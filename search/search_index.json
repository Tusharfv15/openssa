{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenSSM \u2013 \u201cSmall Specialist Models\u201d for Industrial AI See full documentation at aitomatic.github.io/openssm/ . OpenSSM (pronounced open-ess-ess-em ) is an open-source framework for Small Specialist Models (SSMs), which are key to enhancing trust, reliability, and safety in Industrial-AI applications. Harnessing the power of domain expertise, SSMs operate either alone or in \"teams\". They collaborate with other SSMs, planners, and sensors/actuators to deliver real-world problem-solving capabilities. Unlike Large Language Models (LLMs), which are computationally intensive and generalized, SSMs are lean, efficient, and designed specifically for individual domains. This focus makes them an optimal choice for businesses, SMEs, researchers, and developers seeking specialized and robust AI solutions for industrial applications. A prime deployment scenario for SSMs is within the aiCALM (Collaborative Augmented Large Models) architecture. aiCALM represents a cohesive assembly of AI components tailored for sophisticated problem-solving capabilities. Within this framework, SSMs work with General Management Models (GMMs) and other components to solve complex, domain-specific, and industrial problems. Why SSM? The trend towards specialization in AI models is a clear trajectory seen by many in the field. Specialization is crucial for quality .. not general purpose Al models \u2013 Eric Schmidt, Schmidt Foundation .. small models .. for a specific task that are good \u2013 Matei Zaharia, Databricks .. small agents working together .. specific and best in their tasks \u2013 Harrison Chase, Langchain .. small but highly capable expert models \u2013 Andrej Karpathy, OpenAI .. small models are .. a massive paradigm shift .. about deploying AI models at scale \u2013 Rob Toews, Radical Ventures As predicted by Eric Schmidt and others, we will see \u201ca rich ecosystem to emerge [of] high-value, specialized AI systems.\u201d SSMs are the central part in the architecture of these systems. What OpenSSM Offers OpenSSM fills this gap directly, with the following benefits to the community, developers, and businesses: Industrial Focus: SSMs are developed with a specific emphasis on industrial applications, addressing the unique requirements of trustworthiness, safety, reliability, and scalability inherent to this sector. Fast, Cost-Effective & Easy to Use: SSMs are 100-1000x faster and more efficient than LLMs, making them accessible and cost-effective particularly for industrial usage where time and resources are critical factors. Easy Knowledge Capture: OpenSSM has easy-to-use tools for capturing domain knowledge in diverse forms: books, operaring manuals, databases, knowledge graphs, text files, and code. Powerful Operations on Captured Knowledge: OpenSSM enables both knowledge query and inferencing/predictive capabilities based on the domain-specific knowledge. Collaborative Problem-Solving : SSMs are designed to work in problem-solving \"teams\". Multi-SSM collaboration is a first-class design feature, not an afterthought. Reliable Domain Expertise: Each SSM has expertise in a particular field or equipment, offering precise and specialized knowledge, thereby enhancing trustworthiness, reliability, and safety for Industrial-AI applications. With self-reasoning, causal reasoning, and retrieval-based knowledge, SSMs provide a trustable source of domain expertise. Vendor Independence: OpenSSM allows everyone to build, train, and deploy their own domain-expert AI models, offering freedom from vendor lock-in and security concerns. Composable Expertise : SSMs are fully composable, making it easy to combine domain expertise. Target Audience Our primary audience includes: Businesses and SMEs wishing to leverage AI in their specific industrial context without relying on extensive computational resources or large vendor solutions. AI researchers and developers keen on creating more efficient, robust, and domain-specific AI models for industrial applications. Open-source contributors believing in democratizing industrial AI and eager to contribute to a community-driven project focused on building and sharing specialized AI models. Industries with specific domain problems that can be tackled more effectively by a specialist AI model, enhancing the reliability and trustworthiness of AI solutions in an industrial setting. SSM Architecture At a high level, SSMs comprise a front-end Small Language Model (SLM), an adapter layer in the middle, and a wide range of back-end domain-knowledge sources. The SLM itself is a small, efficient, language model, which may be domain-specific or not, and may have been distilled from a larger model. Thus, domain knowledge may come from either, or both, the SLM and the backends. The above diagram illustrates the high-level architecture of an SSM, which comprises three main components: Small Language Model (SLM): This forms the communication frontend of an SSM. Adapters (e.g., LlamaIndex): These provide the interface between the SLM and the domain-knowledge backends. Domain-Knowledge Backends: These include text files, documents, PDFs, databases, code, knowledge graphs, models, other SSMs, etc. SSMs communicate in both unstructured (natural language) and structured APIs, catering to a variety of real-world industrial systems. The composable nature of SSMs allows for easy combination of domain-knowledge sources from multiple models. Getting Started See our Getting Started Guide for more information. Roadmap Play with SSMs in a hosted SSM sandbox, uploading your own domain knowledge Create SSMs in your own development environment, and integrate SSMs into your own AI apps Capture domain knowledge in various forms into your SSMs Train SLMs via distillation of LLMs, teacher/student approaches, etc. Apply SSMs in collaborative problem-solving AI systems Community Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions . Contribute OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. License OpenSSM is released under the Apache 2.0 License .","title":"Home"},{"location":"#openssm-small-specialist-models-for-industrial-ai","text":"See full documentation at aitomatic.github.io/openssm/ . OpenSSM (pronounced open-ess-ess-em ) is an open-source framework for Small Specialist Models (SSMs), which are key to enhancing trust, reliability, and safety in Industrial-AI applications. Harnessing the power of domain expertise, SSMs operate either alone or in \"teams\". They collaborate with other SSMs, planners, and sensors/actuators to deliver real-world problem-solving capabilities. Unlike Large Language Models (LLMs), which are computationally intensive and generalized, SSMs are lean, efficient, and designed specifically for individual domains. This focus makes them an optimal choice for businesses, SMEs, researchers, and developers seeking specialized and robust AI solutions for industrial applications. A prime deployment scenario for SSMs is within the aiCALM (Collaborative Augmented Large Models) architecture. aiCALM represents a cohesive assembly of AI components tailored for sophisticated problem-solving capabilities. Within this framework, SSMs work with General Management Models (GMMs) and other components to solve complex, domain-specific, and industrial problems.","title":"OpenSSM \u2013 \u201cSmall Specialist Models\u201d for Industrial AI"},{"location":"#why-ssm","text":"The trend towards specialization in AI models is a clear trajectory seen by many in the field. Specialization is crucial for quality .. not general purpose Al models \u2013 Eric Schmidt, Schmidt Foundation .. small models .. for a specific task that are good \u2013 Matei Zaharia, Databricks .. small agents working together .. specific and best in their tasks \u2013 Harrison Chase, Langchain .. small but highly capable expert models \u2013 Andrej Karpathy, OpenAI .. small models are .. a massive paradigm shift .. about deploying AI models at scale \u2013 Rob Toews, Radical Ventures As predicted by Eric Schmidt and others, we will see \u201ca rich ecosystem to emerge [of] high-value, specialized AI systems.\u201d SSMs are the central part in the architecture of these systems.","title":"Why SSM?"},{"location":"#what-openssm-offers","text":"OpenSSM fills this gap directly, with the following benefits to the community, developers, and businesses: Industrial Focus: SSMs are developed with a specific emphasis on industrial applications, addressing the unique requirements of trustworthiness, safety, reliability, and scalability inherent to this sector. Fast, Cost-Effective & Easy to Use: SSMs are 100-1000x faster and more efficient than LLMs, making them accessible and cost-effective particularly for industrial usage where time and resources are critical factors. Easy Knowledge Capture: OpenSSM has easy-to-use tools for capturing domain knowledge in diverse forms: books, operaring manuals, databases, knowledge graphs, text files, and code. Powerful Operations on Captured Knowledge: OpenSSM enables both knowledge query and inferencing/predictive capabilities based on the domain-specific knowledge. Collaborative Problem-Solving : SSMs are designed to work in problem-solving \"teams\". Multi-SSM collaboration is a first-class design feature, not an afterthought. Reliable Domain Expertise: Each SSM has expertise in a particular field or equipment, offering precise and specialized knowledge, thereby enhancing trustworthiness, reliability, and safety for Industrial-AI applications. With self-reasoning, causal reasoning, and retrieval-based knowledge, SSMs provide a trustable source of domain expertise. Vendor Independence: OpenSSM allows everyone to build, train, and deploy their own domain-expert AI models, offering freedom from vendor lock-in and security concerns. Composable Expertise : SSMs are fully composable, making it easy to combine domain expertise.","title":"What OpenSSM Offers"},{"location":"#target-audience","text":"Our primary audience includes: Businesses and SMEs wishing to leverage AI in their specific industrial context without relying on extensive computational resources or large vendor solutions. AI researchers and developers keen on creating more efficient, robust, and domain-specific AI models for industrial applications. Open-source contributors believing in democratizing industrial AI and eager to contribute to a community-driven project focused on building and sharing specialized AI models. Industries with specific domain problems that can be tackled more effectively by a specialist AI model, enhancing the reliability and trustworthiness of AI solutions in an industrial setting.","title":"Target Audience"},{"location":"#ssm-architecture","text":"At a high level, SSMs comprise a front-end Small Language Model (SLM), an adapter layer in the middle, and a wide range of back-end domain-knowledge sources. The SLM itself is a small, efficient, language model, which may be domain-specific or not, and may have been distilled from a larger model. Thus, domain knowledge may come from either, or both, the SLM and the backends. The above diagram illustrates the high-level architecture of an SSM, which comprises three main components: Small Language Model (SLM): This forms the communication frontend of an SSM. Adapters (e.g., LlamaIndex): These provide the interface between the SLM and the domain-knowledge backends. Domain-Knowledge Backends: These include text files, documents, PDFs, databases, code, knowledge graphs, models, other SSMs, etc. SSMs communicate in both unstructured (natural language) and structured APIs, catering to a variety of real-world industrial systems. The composable nature of SSMs allows for easy combination of domain-knowledge sources from multiple models.","title":"SSM Architecture"},{"location":"#getting-started","text":"See our Getting Started Guide for more information.","title":"Getting Started"},{"location":"#roadmap","text":"Play with SSMs in a hosted SSM sandbox, uploading your own domain knowledge Create SSMs in your own development environment, and integrate SSMs into your own AI apps Capture domain knowledge in various forms into your SSMs Train SLMs via distillation of LLMs, teacher/student approaches, etc. Apply SSMs in collaborative problem-solving AI systems","title":"Roadmap"},{"location":"#community","text":"Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions .","title":"Community"},{"location":"#contribute","text":"OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details.","title":"Contribute"},{"location":"#license","text":"OpenSSM is released under the Apache 2.0 License .","title":"License"},{"location":"GETTING_STARTED/","text":"Getting Started with OpenSSM Who Are You? An end-user of OpenSSM-based applications A developer of applications or services using OpenSSM An aspiring contributor to OpenSSM A committer to OpenSSM Getting Started as an End-User Getting Started as a Developer See some example user programs in the examples directory. For example, to run the chatssm example, do: % cd examples/chatssm % make clean % make Common make targets for OpenSSM developers See MAKEFILE for more details. % make clean % make build % make rebuild % make test % make poetry-init % make poetry-install % make install # local installation of openssm % make pypi-auth # only for maintainers % make publish # only for maintainers Getting Started as an Aspiring Contributor OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. You can begin contributing to the OpenSSM project in the contrib/ directory. Getting Started as a Committer You already know what to do. Community Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions . License OpenSSM is released under the Apache 2.0 License . Links MAKEFILE","title":"Getting Started"},{"location":"GETTING_STARTED/#getting-started-with-openssm","text":"","title":"Getting Started with OpenSSM"},{"location":"GETTING_STARTED/#who-are-you","text":"An end-user of OpenSSM-based applications A developer of applications or services using OpenSSM An aspiring contributor to OpenSSM A committer to OpenSSM","title":"Who Are You?"},{"location":"GETTING_STARTED/#getting-started-as-an-end-user","text":"","title":"Getting Started as an End-User"},{"location":"GETTING_STARTED/#getting-started-as-a-developer","text":"See some example user programs in the examples directory. For example, to run the chatssm example, do: % cd examples/chatssm % make clean % make","title":"Getting Started as a Developer"},{"location":"GETTING_STARTED/#common-make-targets-for-openssm-developers","text":"See MAKEFILE for more details. % make clean % make build % make rebuild % make test % make poetry-init % make poetry-install % make install # local installation of openssm % make pypi-auth # only for maintainers % make publish # only for maintainers","title":"Common make targets for OpenSSM developers"},{"location":"GETTING_STARTED/#getting-started-as-an-aspiring-contributor","text":"OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. You can begin contributing to the OpenSSM project in the contrib/ directory.","title":"Getting Started as an Aspiring Contributor"},{"location":"GETTING_STARTED/#getting-started-as-a-committer","text":"You already know what to do.","title":"Getting Started as a Committer"},{"location":"GETTING_STARTED/#community","text":"Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions .","title":"Community"},{"location":"GETTING_STARTED/#license","text":"OpenSSM is released under the Apache 2.0 License .","title":"License"},{"location":"GETTING_STARTED/#links","text":"MAKEFILE","title":"Links"},{"location":"LICENSE/","text":"OpenSSM Project License OpenSSM is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project's files except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"LICENSE/#openssm-project-license","text":"OpenSSM is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project's files except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"OpenSSM Project License"},{"location":"PROJECT_PHILOSOPHY/","text":"OpenSSM Project Philosophy At OpenSSM, we believe in the democratization of AI. Our goal is to create an ecosystem where anyone, regardless of their resources, can have access to efficient and domain-specific AI solutions. We envision a future where AI is not only accessible but also robust, reliable, and trustworthy. Our project is guided by the following principles: Collaboration: We aim to foster an environment of collaboration where multiple models can work together to solve complex problems. Empowerment: We strive to empower enterprises, SMEs, and individuals to build, train, and deploy their own AI models. Inclusivity: We are committed to creating a project that welcomes and includes contributions from everyone, regardless of their background, expertise, or resources. Transparency: We believe in open-source and the power of shared knowledge. Our code, our models, and our development processes are transparent and open to all. Excellence: We continuously strive for the highest standards in our models, ensuring they are efficient, reliable, and precise in their domain-specific knowledge. Our community is our greatest strength, and we are committed to nurturing it with these values in mind.","title":"Project Philosophy"},{"location":"PROJECT_PHILOSOPHY/#openssm-project-philosophy","text":"At OpenSSM, we believe in the democratization of AI. Our goal is to create an ecosystem where anyone, regardless of their resources, can have access to efficient and domain-specific AI solutions. We envision a future where AI is not only accessible but also robust, reliable, and trustworthy. Our project is guided by the following principles: Collaboration: We aim to foster an environment of collaboration where multiple models can work together to solve complex problems. Empowerment: We strive to empower enterprises, SMEs, and individuals to build, train, and deploy their own AI models. Inclusivity: We are committed to creating a project that welcomes and includes contributions from everyone, regardless of their background, expertise, or resources. Transparency: We believe in open-source and the power of shared knowledge. Our code, our models, and our development processes are transparent and open to all. Excellence: We continuously strive for the highest standards in our models, ensuring they are efficient, reliable, and precise in their domain-specific knowledge. Our community is our greatest strength, and we are committed to nurturing it with these values in mind.","title":"OpenSSM Project Philosophy"},{"location":"community/CODE_OF_CONDUCT/","text":"Code of Conduct This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in OpenSSM to help us create safe and positive experiences for everyone. Expected Behavior The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence, or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist, or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Consequences of Unacceptable Behavior Unacceptable behavior from any community member will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning. Reporting Guidelines If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. Addressing Grievances If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify the project team with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Scope We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business.","title":"Code of Conduct"},{"location":"community/CODE_OF_CONDUCT/#code-of-conduct","text":"This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in OpenSSM to help us create safe and positive experiences for everyone.","title":"Code of Conduct"},{"location":"community/CODE_OF_CONDUCT/#expected-behavior","text":"The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech.","title":"Expected Behavior"},{"location":"community/CODE_OF_CONDUCT/#unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence, or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist, or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability.","title":"Unacceptable Behavior"},{"location":"community/CODE_OF_CONDUCT/#consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any community member will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning.","title":"Consequences of Unacceptable Behavior"},{"location":"community/CODE_OF_CONDUCT/#reporting-guidelines","text":"If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible.","title":"Reporting Guidelines"},{"location":"community/CODE_OF_CONDUCT/#addressing-grievances","text":"If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify the project team with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies.","title":"Addressing Grievances"},{"location":"community/CODE_OF_CONDUCT/#scope","text":"We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business.","title":"Scope"},{"location":"community/CONTRIBUTING/","text":"Contributing to OpenSSM Thanks for your interest in contributing to OpenSSM! This document provides guidelines for contributing to the project. Please read these guidelines before submitting a contribution. Code of Conduct All contributors must abide by the Code of Conduct . Please read it before contributing. How to Contribute Find an issue to work on: Look at the list of open issues in the OpenSSM repository. Pick one that interests you and that no one else is working on. Fork the repository and create a branch: If you're not a project maintainer, you'll need to create a fork of the repository and create a branch on your fork where you can make your changes. Submit a pull request: After you've made your changes, submit a pull request to merge your branch into the main OpenSSM repository. Be sure to link the issue you're addressing in your pull request. Please ensure your contribution meets the following guidelines: Code contributions must be compatible with the project's license. We follow PEP8 for Python style guidelines. Include tests when adding new features. Contributions will not be accepted without them. Document new code based on the documentation style guide. Make sure all tests pass on your machine before you submit a pull request. Thank you for your contributions!","title":"Contributing"},{"location":"community/CONTRIBUTING/#contributing-to-openssm","text":"Thanks for your interest in contributing to OpenSSM! This document provides guidelines for contributing to the project. Please read these guidelines before submitting a contribution.","title":"Contributing to OpenSSM"},{"location":"community/CONTRIBUTING/#code-of-conduct","text":"All contributors must abide by the Code of Conduct . Please read it before contributing.","title":"Code of Conduct"},{"location":"community/CONTRIBUTING/#how-to-contribute","text":"Find an issue to work on: Look at the list of open issues in the OpenSSM repository. Pick one that interests you and that no one else is working on. Fork the repository and create a branch: If you're not a project maintainer, you'll need to create a fork of the repository and create a branch on your fork where you can make your changes. Submit a pull request: After you've made your changes, submit a pull request to merge your branch into the main OpenSSM repository. Be sure to link the issue you're addressing in your pull request. Please ensure your contribution meets the following guidelines: Code contributions must be compatible with the project's license. We follow PEP8 for Python style guidelines. Include tests when adding new features. Contributions will not be accepted without them. Document new code based on the documentation style guide. Make sure all tests pass on your machine before you submit a pull request. Thank you for your contributions!","title":"How to Contribute"},{"location":"dev/design_principles/","text":"OpenSSM Design Principles Specialization Over Generalization: Our models are designed to be domain-specific to provide precise solutions to specific problems, rather than providing generalized solutions. Efficiency and Speed: We aim for our models to be faster and more efficient than large language models, making AI more accessible and cost-effective. Trustworthiness and Reliability: As a foundation of industrial AI, our models are developed with an emphasis on robustness, reliability, and scalability. Collaborative Approach: We believe in the power of combined intelligence. Our models can be deployed together to solve complex problems. Community-driven: Our models are developed by the community, for the community. We welcome contributions from everyone, regardless of their background or expertise.","title":"Design Principles"},{"location":"dev/design_principles/#openssm-design-principles","text":"Specialization Over Generalization: Our models are designed to be domain-specific to provide precise solutions to specific problems, rather than providing generalized solutions. Efficiency and Speed: We aim for our models to be faster and more efficient than large language models, making AI more accessible and cost-effective. Trustworthiness and Reliability: As a foundation of industrial AI, our models are developed with an emphasis on robustness, reliability, and scalability. Collaborative Approach: We believe in the power of combined intelligence. Our models can be deployed together to solve complex problems. Community-driven: Our models are developed by the community, for the community. We welcome contributions from everyone, regardless of their background or expertise.","title":"OpenSSM Design Principles"},{"location":"dev/howtos/","text":"Helpful How-Tos Observability OpenSSM has built-in observability and tracing. Logging Users of OpenSSM can use the logger object provided by the OpenSSM package: from OpenSSM import logger logger.warning(\"xyz = %s\", xyz) If you are an OpenSSM contributor, you may use the openssm logger: from openssm import mlogger mlogger.warning(\"xyz = %s\", xyz) Automatic function logging There are some useful decorators for automatically logging function entry and exit. from openssm import Logs @Logs.do_log_entry_and_exit() # upon both entry and exit def func(param1, param2): @Logs.do_log_entry() # only upon entry @Logs.do_log_exit() # only upon exit The above will automatically log function entry with its parameters, and function exit with its return value. If you want to use your own logger with its own name, use from openssm import Logs my_logger = Logs.get_logger(app_name, logger.INFO) @Logs.do_log_entry_and_exit(logger=my_logger) def func(param1, param2): Sometimes it is useful to be able to specify additional parameters to be logged: @Logs.do_log_entry_and_exit({'request': request})","title":"Other HowTos"},{"location":"dev/howtos/#helpful-how-tos","text":"","title":"Helpful How-Tos"},{"location":"dev/howtos/#observability","text":"OpenSSM has built-in observability and tracing.","title":"Observability"},{"location":"dev/howtos/#logging","text":"Users of OpenSSM can use the logger object provided by the OpenSSM package: from OpenSSM import logger logger.warning(\"xyz = %s\", xyz) If you are an OpenSSM contributor, you may use the openssm logger: from openssm import mlogger mlogger.warning(\"xyz = %s\", xyz)","title":"Logging"},{"location":"dev/howtos/#automatic-function-logging","text":"There are some useful decorators for automatically logging function entry and exit. from openssm import Logs @Logs.do_log_entry_and_exit() # upon both entry and exit def func(param1, param2): @Logs.do_log_entry() # only upon entry @Logs.do_log_exit() # only upon exit The above will automatically log function entry with its parameters, and function exit with its return value. If you want to use your own logger with its own name, use from openssm import Logs my_logger = Logs.get_logger(app_name, logger.INFO) @Logs.do_log_entry_and_exit(logger=my_logger) def func(param1, param2): Sometimes it is useful to be able to specify additional parameters to be logged: @Logs.do_log_entry_and_exit({'request': request})","title":"Automatic function logging"},{"location":"dev/makefile_info/","text":"Makefile guide We use Makefiles extensively to help make the developer\u2019s life simpler and more efficient. Here are the key targets for the top-level Makefile . dev-setup : run this first to set up your dev environment. test : perform testing on both Python and JS code found. test-console : same as test , but also show all output on the console. lint : run pylint and eslint on the code base. pre-commit : perform both linting and testing prior to commits, or at least pull requests. build : build the library (using poetry). install : build and perform a pip install from the local .whl outputs. clean : remove all the start from a clean slate. publish : publish the .whl to Pypi (for pip install support). pypi-auth : convenient target to set up your Pypi auth token prior to publishing docs-build : build web-based documentation docs-deploy : deploy web-based documentation to GitHub, e.g., aitomatic.github.io/openssm Miscellaneous: internal use or sub-targets Links GETTING STARTED","title":"Using Makefile"},{"location":"dev/makefile_info/#makefile-guide","text":"We use Makefiles extensively to help make the developer\u2019s life simpler and more efficient. Here are the key targets for the top-level Makefile . dev-setup : run this first to set up your dev environment. test : perform testing on both Python and JS code found. test-console : same as test , but also show all output on the console. lint : run pylint and eslint on the code base. pre-commit : perform both linting and testing prior to commits, or at least pull requests. build : build the library (using poetry). install : build and perform a pip install from the local .whl outputs. clean : remove all the start from a clean slate. publish : publish the .whl to Pypi (for pip install support). pypi-auth : convenient target to set up your Pypi auth token prior to publishing docs-build : build web-based documentation docs-deploy : deploy web-based documentation to GitHub, e.g., aitomatic.github.io/openssm Miscellaneous: internal use or sub-targets","title":"Makefile guide"},{"location":"dev/makefile_info/#links","text":"GETTING STARTED","title":"Links"},{"location":"diagrams/","text":"Design Diagrams ssm.drawio ssm-class-diagram.drawio.png ssm-composability.drawio.png ssm-full-industrial-use-case.drawio.png ssm-industrial-use-case.drawio.png ssm-key-components.drawio.png ssm-llama-index-integration.drawio.png","title":"Diagrams"},{"location":"diagrams/#design-diagrams","text":"ssm.drawio ssm-class-diagram.drawio.png ssm-composability.drawio.png ssm-full-industrial-use-case.drawio.png ssm-industrial-use-case.drawio.png ssm-key-components.drawio.png ssm-llama-index-integration.drawio.png","title":"Design Diagrams"},{"location":"integrations/lepton_ai/","text":"Lepton.AI Integration Lepton.AI is a developer-centric platform to build, fine-tune, and deploy large models. With OpenSSM, you can create SSMs by calling the Lepton pipeline with just a few lines of code. from openssm import BaseSSM, LeptonSLMFactory ssm = BaseSSM(slm=LeptonSLMFactory.create()) response = ssm.discuss(conversation_id, \"what is abc?\") Integration Architecture In the OpenSSM context, Lepton helps finetune and distill the SLM (small language model) that front-ends an SSM. ![Lepton Integration](../diagrams/ssm-lepton-integration.drawio.png) Roadmap","title":"Lepton.AI"},{"location":"integrations/lepton_ai/#leptonai-integration","text":"Lepton.AI is a developer-centric platform to build, fine-tune, and deploy large models. With OpenSSM, you can create SSMs by calling the Lepton pipeline with just a few lines of code. from openssm import BaseSSM, LeptonSLMFactory ssm = BaseSSM(slm=LeptonSLMFactory.create()) response = ssm.discuss(conversation_id, \"what is abc?\")","title":"Lepton.AI Integration"},{"location":"integrations/lepton_ai/#integration-architecture","text":"In the OpenSSM context, Lepton helps finetune and distill the SLM (small language model) that front-ends an SSM. ![Lepton Integration](../diagrams/ssm-lepton-integration.drawio.png)","title":"Integration Architecture"},{"location":"integrations/lepton_ai/#roadmap","text":"","title":"Roadmap"},{"location":"integrations/vectara/","text":"Vectara Integration Vectara is a developer-first API platform for easily building conversational search experiences that feature best-in-class Retrieval, Summarization, and \u201cGrounded Generation\u201d that all but eliminates hallucinations. With OpenSSM, you can simply use Vectara with just a few lines of code. from openssm import VectaraSSM ssm = VectaraSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\") Integration Architecture In the OpenSSM context, Vectara is treated as a backend, as shown below.. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the Vectara backend. Roadmap","title":"Vectara"},{"location":"integrations/vectara/#vectara-integration","text":"Vectara is a developer-first API platform for easily building conversational search experiences that feature best-in-class Retrieval, Summarization, and \u201cGrounded Generation\u201d that all but eliminates hallucinations. With OpenSSM, you can simply use Vectara with just a few lines of code. from openssm import VectaraSSM ssm = VectaraSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\")","title":"Vectara Integration"},{"location":"integrations/vectara/#integration-architecture","text":"In the OpenSSM context, Vectara is treated as a backend, as shown below.. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the Vectara backend.","title":"Integration Architecture"},{"location":"integrations/vectara/#roadmap","text":"","title":"Roadmap"},{"location":"openssm/capture/EMPTY/","text":"This directory is (still) empty.","title":"capture"},{"location":"openssm/composer/EMPTY/","text":"This directory is (still) empty.","title":"composer"},{"location":"openssm/contrib/apps/EMPTY/","text":"This directory is (still) empty.","title":"apps"},{"location":"openssm/contrib/ssms/industrial_boilers_ssm/EMPTY/","text":"This directory is (still) empty.","title":"industrial_boilers_ssm"},{"location":"openssm/contrib/ssms/japan_fish_kcp_ssm/EMPTY/","text":"This directory is (still) empty.","title":"japan_fish_kcp_ssm"},{"location":"openssm/contrib/ssms/mri_operator_ssm/EMPTY/","text":"This directory is (still) empty.","title":"mri_operator_ssm"},{"location":"openssm/contrib/ssms/semiconductor_ssm/EMPTY/","text":"This directory is (still) empty.","title":"semiconductor_ssm"},{"location":"openssm/core/prompts/","text":"","title":"prompts"},{"location":"openssm/core/adapter/abstract_adapter/","text":"AbstractAdapter dataclass Bases: ABC The AbstractAdapter serves as the base for all concrete Adapter classes. It provides an interface for interaction between the Small Language Model (SLM) and the Backend. backends : list [ AbstractBackend ] abstractmethod property writable Returns our backends facts : list [ str ] abstractmethod property Lists all known facts. heuristics abstractmethod property Lists all known heuristics. inferencers abstractmethod property Lists all known inferencers. add_backend ( backend ) abstractmethod Adds a backend to our adapter enumerate_backends ( lambda_function ) Enumerate backends and apply lambda function to each backend. load ( storage_dir ) abstractmethod Loads from the specified directory. query_all ( user_input , conversation = None ) abstractmethod Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response. save ( storage_dir ) abstractmethod Saves to the specified directory. select_facts ( criteria ) abstractmethod Selects or searches for facts based on provided criteria. select_heuristics ( criteria ) abstractmethod Selects or searches for heuristics based on provided criteria. select_inferencers ( criteria ) abstractmethod Selects or searches for inferencers based on provided criteria.","title":"abstract_adapter"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter","text":"Bases: ABC The AbstractAdapter serves as the base for all concrete Adapter classes. It provides an interface for interaction between the Small Language Model (SLM) and the Backend.","title":"AbstractAdapter"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.backends","text":"Returns our backends","title":"backends"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.facts","text":"Lists all known facts.","title":"facts"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.heuristics","text":"Lists all known heuristics.","title":"heuristics"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.inferencers","text":"Lists all known inferencers.","title":"inferencers"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.add_backend","text":"Adds a backend to our adapter","title":"add_backend()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.enumerate_backends","text":"Enumerate backends and apply lambda function to each backend.","title":"enumerate_backends()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.query_all","text":"Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response.","title":"query_all()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_facts","text":"Selects or searches for facts based on provided criteria.","title":"select_facts()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_heuristics","text":"Selects or searches for heuristics based on provided criteria.","title":"select_heuristics()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_inferencers","text":"Selects or searches for inferencers based on provided criteria.","title":"select_inferencers()"},{"location":"openssm/core/adapter/base_adapter/","text":"BaseAdapter Bases: AbstractAdapter Base adapter class for SSMs. backends : list [ AbstractBackend ] property writable Side effect: if no backends are set, a default TextBackend is created. facts property List facts from all backends. heuristics property List heuristics from all backends. inferencers property List inferencers from all backends. add_backend ( backend ) Add a backend to the list of backends. add_fact ( fact ) Idiom: add a fact to the first backend we have. enumerate_backends ( lambda_function ) Enumerate backends and apply lambda function to each backend. load ( storage_dir ) Loads from the specified directory. query_all ( user_input , conversation = None ) Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's responses save ( storage_dir ) Saves to the specified directory. select_facts ( criteria ) Select facts from all backends. select_heuristics ( criteria ) Select heuristics from all backends. select_inferencers ( criteria ) Select inferencers from all backends.","title":"base_adapter"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter","text":"Bases: AbstractAdapter Base adapter class for SSMs.","title":"BaseAdapter"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.backends","text":"Side effect: if no backends are set, a default TextBackend is created.","title":"backends"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.facts","text":"List facts from all backends.","title":"facts"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.heuristics","text":"List heuristics from all backends.","title":"heuristics"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.inferencers","text":"List inferencers from all backends.","title":"inferencers"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.add_backend","text":"Add a backend to the list of backends.","title":"add_backend()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.add_fact","text":"Idiom: add a fact to the first backend we have.","title":"add_fact()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.enumerate_backends","text":"Enumerate backends and apply lambda function to each backend.","title":"enumerate_backends()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.query_all","text":"Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's responses","title":"query_all()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_facts","text":"Select facts from all backends.","title":"select_facts()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_heuristics","text":"Select heuristics from all backends.","title":"select_heuristics()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_inferencers","text":"Select inferencers from all backends.","title":"select_inferencers()"},{"location":"openssm/core/backend/abstract_backend/","text":"AbstractBackend dataclass Bases: ABC facts abstractmethod property Returns a set of facts. heuristics abstractmethod property Returns a set of heuristics. inferencers abstractmethod property Returns a set of inferencers. add_fact ( fact ) abstractmethod Adds a fact to the backend. add_heuristic ( heuristic ) abstractmethod Adds a heuristic to the backend. add_inferencer ( inferencer ) abstractmethod Adds an inferencer to the backend. load ( storage_dir ) abstractmethod Loads from the specified directory. load_all () abstractmethod Loads all facts, inferencers, and heuristics, if appropriate. Some backends may not need to, and only load on demand (e.g., a database backend). query ( user_input , conversation = None ) abstractmethod Queries the backend with the user input. Response may be in the form {\"response\": \"some response\", \"response_object\": some_object} save ( storage_dir ) abstractmethod Saves to the specified directory. select_facts ( criteria ) abstractmethod Returns a set of facts that match the criteria. select_heuristics ( criteria ) abstractmethod Returns a set of heuristics that match the criteria. select_inferencers ( criteria ) abstractmethod Returns a set of inferencers that match the criteria.","title":"abstract_backend"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend","text":"Bases: ABC","title":"AbstractBackend"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.facts","text":"Returns a set of facts.","title":"facts"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.heuristics","text":"Returns a set of heuristics.","title":"heuristics"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.inferencers","text":"Returns a set of inferencers.","title":"inferencers"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.add_fact","text":"Adds a fact to the backend.","title":"add_fact()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.add_heuristic","text":"Adds a heuristic to the backend.","title":"add_heuristic()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.add_inferencer","text":"Adds an inferencer to the backend.","title":"add_inferencer()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.load_all","text":"Loads all facts, inferencers, and heuristics, if appropriate. Some backends may not need to, and only load on demand (e.g., a database backend).","title":"load_all()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.query","text":"Queries the backend with the user input. Response may be in the form {\"response\": \"some response\", \"response_object\": some_object}","title":"query()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.select_facts","text":"Returns a set of facts that match the criteria.","title":"select_facts()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.select_heuristics","text":"Returns a set of heuristics that match the criteria.","title":"select_heuristics()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.select_inferencers","text":"Returns a set of inferencers that match the criteria.","title":"select_inferencers()"},{"location":"openssm/core/backend/base_backend/","text":"BaseBackend Bases: AbstractBackend load ( storage_dir ) Loads from the specified directory. load_all () The base backend does not load anything. It gets all its facts, inferencers, and heuristics through the add_* methods. query ( user_input , conversation = None ) Backends are expected to return a dict with the following keys: - response: a string - response_object: an object that has a lot more information about the response save ( storage_dir ) Saves to the specified directory. select_facts ( criteria ) The base backend simply returns all facts. select_heuristics ( criteria ) The base backend simply returns all heuristics. select_inferencers ( criteria ) The base backend simply returns all inferencers.","title":"base_backend"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend","text":"Bases: AbstractBackend","title":"BaseBackend"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.load_all","text":"The base backend does not load anything. It gets all its facts, inferencers, and heuristics through the add_* methods.","title":"load_all()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.query","text":"Backends are expected to return a dict with the following keys: - response: a string - response_object: an object that has a lot more information about the response","title":"query()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_facts","text":"The base backend simply returns all facts.","title":"select_facts()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_heuristics","text":"The base backend simply returns all heuristics.","title":"select_heuristics()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_inferencers","text":"The base backend simply returns all inferencers.","title":"select_inferencers()"},{"location":"openssm/core/backend/rag_backend/","text":"AbstractRAGBackend Bases: BaseBackend , ABC load ( storage_dir ) Load the index from the storage directory. @param storage_dir: The path to the base storage directory. read_directory ( storage_dir , use_existing_index = True ) Read a directory of documents and create an index. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use) read_gdrive ( folder_id , storage_dir , use_existing_index = True ) Read a directory of documents from a Google Drive folder and create an index. Internally, the documents will first be downloaded to a local directory. @param folder_id: The ID of the Google Drive folder. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use) read_website ( urls , storage_dir , use_existing_index = True ) Read a directory of documents from a website and create an index. Internally, no documents are downloaded to a local directory. @param url: The URL of the website. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use) save ( storage_dir ) Save the index to the storage directory. @param storage_dir: The path to the base storage directory.","title":"rag_backend"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend","text":"Bases: BaseBackend , ABC","title":"AbstractRAGBackend"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend.load","text":"Load the index from the storage directory. @param storage_dir: The path to the base storage directory.","title":"load()"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend.read_directory","text":"Read a directory of documents and create an index. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use)","title":"read_directory()"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend.read_gdrive","text":"Read a directory of documents from a Google Drive folder and create an index. Internally, the documents will first be downloaded to a local directory. @param folder_id: The ID of the Google Drive folder. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use)","title":"read_gdrive()"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend.read_website","text":"Read a directory of documents from a website and create an index. Internally, no documents are downloaded to a local directory. @param url: The URL of the website. @param storage_dir: The path to the base storage directory. @param use_existing_index: [optional] If True, try to load an existing index from the storage directory first. Side effects: - If use_existing_index is True, the index will be automatically saved (for future use)","title":"read_website()"},{"location":"openssm/core/backend/rag_backend/#openssm.core.backend.rag_backend.AbstractRAGBackend.save","text":"Save the index to the storage directory. @param storage_dir: The path to the base storage directory.","title":"save()"},{"location":"openssm/core/backend/text_backend/","text":"","title":"text_backend"},{"location":"openssm/core/inferencer/abstract_inferencer/","text":"AbstractInferencer is the base class for all inferencers. AbstractInferencer dataclass Bases: ABC The AbstractInferencer serves as the base for all concrete Inferencer classes. The most common inferencer is simply an ML model, but it could also be a rule-based system, a fuzzy logic system, or any other system that can infer a response from a given input. load ( path ) abstractmethod Loads the inferencer or its parameters from the given path. predict ( input_data ) abstractmethod Returns a prediction based on the given input.","title":"abstract_inferencer"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer","text":"Bases: ABC The AbstractInferencer serves as the base for all concrete Inferencer classes. The most common inferencer is simply an ML model, but it could also be a rule-based system, a fuzzy logic system, or any other system that can infer a response from a given input.","title":"AbstractInferencer"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer.load","text":"Loads the inferencer or its parameters from the given path.","title":"load()"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer.predict","text":"Returns a prediction based on the given input.","title":"predict()"},{"location":"openssm/core/inferencer/base_inferencer/","text":"BaseInferencer Bases: AbstractInferencer load ( path ) The BaseInferencer does not need to load anything. predict ( input_data ) The BaseInferencer always returns a prediction of True.","title":"base_inferencer"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer","text":"Bases: AbstractInferencer","title":"BaseInferencer"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer.load","text":"The BaseInferencer does not need to load anything.","title":"load()"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer.predict","text":"The BaseInferencer always returns a prediction of True.","title":"predict()"},{"location":"openssm/core/slm/abstract_slm/","text":"AbstractSLM dataclass Bases: ABC The AbstractSLM serves as the base for all concrete Small Language Models (SLMs). It provides an interface for natural language communication and structured API interactions. adapter : AbstractAdapter abstractmethod property writable Returns our adapter do_discuss ( user_input , conversation ) abstractmethod Processes a natural language conversation input and returns a dict of the reply. Not intended for direct use. load ( storage_dir ) abstractmethod Loads from the specified directory. reset_memory () abstractmethod Resets our conversation memory save ( storage_dir ) abstractmethod Saves to the specified directory.","title":"abstract_slm"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM","text":"Bases: ABC The AbstractSLM serves as the base for all concrete Small Language Models (SLMs). It provides an interface for natural language communication and structured API interactions.","title":"AbstractSLM"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.adapter","text":"Returns our adapter","title":"adapter"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.do_discuss","text":"Processes a natural language conversation input and returns a dict of the reply. Not intended for direct use.","title":"do_discuss()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.reset_memory","text":"Resets our conversation memory","title":"reset_memory()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/slm/base_slm/","text":"BaseSLM Bases: AbstractSLM adapter : AbstractAdapter property writable Return the previous assigned Adapter, or a default Adapter if none was assigned. conversations : dict property writable Return the previous assigned conversations, or an empty dictionary if none was assigned. __init__ ( adapter = None ) self.conversations is initialized as a dictionary of conversations, where each conversation is a list of user inputs and model replies. do_discuss ( user_input , conversation ) Add the user_input to the conversation, sends the whole conversation to the language model, and returns the reply. load ( storage_dir ) Loads from the specified directory. save ( storage_dir ) Saves to the specified directory. PassthroughSLM Bases: BaseSLM The PassthroughSLM is a barebones SLM that simply passes all queries to the adapter. do_discuss ( user_input , conversation ) Pass through user input to the adapter and return the replies","title":"base_slm"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM","text":"Bases: AbstractSLM","title":"BaseSLM"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.adapter","text":"Return the previous assigned Adapter, or a default Adapter if none was assigned.","title":"adapter"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.conversations","text":"Return the previous assigned conversations, or an empty dictionary if none was assigned.","title":"conversations"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.__init__","text":"self.conversations is initialized as a dictionary of conversations, where each conversation is a list of user inputs and model replies.","title":"__init__()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.do_discuss","text":"Add the user_input to the conversation, sends the whole conversation to the language model, and returns the reply.","title":"do_discuss()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.load","text":"Loads from the specified directory.","title":"load()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.save","text":"Saves to the specified directory.","title":"save()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.PassthroughSLM","text":"Bases: BaseSLM The PassthroughSLM is a barebones SLM that simply passes all queries to the adapter.","title":"PassthroughSLM"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.PassthroughSLM.do_discuss","text":"Pass through user input to the adapter and return the replies","title":"do_discuss()"},{"location":"openssm/core/slm/memory/conversation_db/","text":"","title":"conversation_db"},{"location":"openssm/core/slm/memory/sqlite_conversation_db/","text":"","title":"sqlite_conversation_db"},{"location":"openssm/core/ssm/abstract_ssm/","text":"AbstractSSM dataclass Bases: ABC The AbstractSSM serves as the base for all concrete Small Specialist Models (SSMs). adapter : AbstractAdapter abstractmethod property writable Returns our adapter backends : list [ AbstractBackend ] abstractmethod property writable Returns our backends facts : list [ str ] abstractmethod property Lists all known facts. heuristics : list [ str ] abstractmethod property Lists all known heuristics. inferencers : list [ str ] abstractmethod property Lists all known inferencers. slm : AbstractSLM abstractmethod property writable Returns our small language model (SLM) add_knowledge ( knowledge_source_uri , knowledge_type = None ) abstractmethod Uploads a knowledge source (documents, text, files, etc.) api_call ( function_name , * args , ** kwargs ) abstractmethod Processes a structured API call. discuss ( user_input , conversation_id = None ) abstractmethod Processes a natural language conversation input. infer ( input_facts ) abstractmethod Makes inferences based on the provided input facts. load ( storage_dir ) abstractmethod Loads the SSM from the specified directory. reset_memory () abstractmethod Resets the conversation memory of the SSM. save ( storage_dir ) abstractmethod Saves the SSM to the specified directory. select_facts ( criteria ) abstractmethod Selects or searches for facts based on provided criteria. select_heuristics ( criteria ) abstractmethod Selects or searches for heuristics based on provided criteria. select_inferencers ( criteria ) abstractmethod Selects or searches for inferencers based on provided criteria. solve_problem ( problem_description ) abstractmethod Solves a problem based on the provided description.","title":"abstract_ssm"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM","text":"Bases: ABC The AbstractSSM serves as the base for all concrete Small Specialist Models (SSMs).","title":"AbstractSSM"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.adapter","text":"Returns our adapter","title":"adapter"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.backends","text":"Returns our backends","title":"backends"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.facts","text":"Lists all known facts.","title":"facts"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.heuristics","text":"Lists all known heuristics.","title":"heuristics"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.inferencers","text":"Lists all known inferencers.","title":"inferencers"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.slm","text":"Returns our small language model (SLM)","title":"slm"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.api_call","text":"Processes a structured API call.","title":"api_call()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.discuss","text":"Processes a natural language conversation input.","title":"discuss()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.infer","text":"Makes inferences based on the provided input facts.","title":"infer()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.load","text":"Loads the SSM from the specified directory.","title":"load()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.reset_memory","text":"Resets the conversation memory of the SSM.","title":"reset_memory()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.save","text":"Saves the SSM to the specified directory.","title":"save()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_facts","text":"Selects or searches for facts based on provided criteria.","title":"select_facts()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_heuristics","text":"Selects or searches for heuristics based on provided criteria.","title":"select_heuristics()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_inferencers","text":"Selects or searches for inferencers based on provided criteria.","title":"select_inferencers()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.solve_problem","text":"Solves a problem based on the provided description.","title":"solve_problem()"},{"location":"openssm/core/ssm/abstract_ssm_builder/","text":"AbstractSSMBuilder Bases: ABC add_inferencer ( inferencer , knowledge_id ) abstractmethod Adds or creates an inferencer (e.g., ML models) to a specific knowledge source add_knowledge ( knowledge_source_uri , source_type = None ) abstractmethod Uploads a knowledge source (documents, text, files, etc.), returning knowledge_id create_ssm ( knowledge_ids , model_parameters = None ) abstractmethod Creates an SSM based on the provided knowledge sources and model parameters extract_structured_information ( knowledge_id ) abstractmethod Extracts structured information (facts, heuristics) from a specific knowledge_id generate_training_data ( knowledge_id , prompt_parameters = None ) abstractmethod Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model train_slm ( model , training_data , fine_tuning_parameters = None ) abstractmethod Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"abstract_ssm_builder"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder","text":"Bases: ABC","title":"AbstractSSMBuilder"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.add_inferencer","text":"Adds or creates an inferencer (e.g., ML models) to a specific knowledge source","title":"add_inferencer()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.), returning knowledge_id","title":"add_knowledge()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.create_ssm","text":"Creates an SSM based on the provided knowledge sources and model parameters","title":"create_ssm()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.extract_structured_information","text":"Extracts structured information (facts, heuristics) from a specific knowledge_id","title":"extract_structured_information()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.generate_training_data","text":"Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model","title":"generate_training_data()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.train_slm","text":"Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"train_slm()"},{"location":"openssm/core/ssm/base_ssm/","text":"BaseSSM Bases: AbstractSSM adapter : AbstractAdapter property writable Return the previous assigned Adapter, or a default Adapter if none was assigned. backends : list [ AbstractBackend ] property writable Return the previous assigned backends, or a default backend if none was assigned. conversations : dict property writable Return the previous assigned conversations, or an empty dictionary if none was assigned. facts : list [ str ] property Return the facts from the adapter. heuristics : list [ str ] property Return the heuristics from the adapter. inferencers : list [ str ] property Return the inferencers from the adapter. name : str property writable Return the previous assigned name, or a default name if none was assigned. slm : AbstractSLM property writable Return the previous assigned SLM, or a default SLM if none was assigned. track_conversations : bool property writable Return the previous assigned track_conversations, or a default value if none was assigned. add_knowledge ( knowledge_source_uri , knowledge_type = None ) Uploads a knowledge source (documents, text, files, etc.) custom_discuss ( user_input , conversation ) Send user input to our SLM and return the reply, AND the actual user input. In the base implementation, the user_input is unchanged from what we are given. But derived classes can override this method to do things like: Add other context info to the user_input Query other models first and combine their replies to form a single user_input etc. get_conversation ( conversation_id = None ) Return the conversation with the given id. Instantiate a new conversation if none was found, and an id was given. load ( storage_dir = None ) Loads the SSM from the specified directory. save ( storage_dir = None ) Saves the SSM to the specified directory. update_conversation ( user_input , reply , conversation_id = None ) Update the conversation with the user_input and reply.","title":"base_ssm"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM","text":"Bases: AbstractSSM","title":"BaseSSM"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.adapter","text":"Return the previous assigned Adapter, or a default Adapter if none was assigned.","title":"adapter"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.backends","text":"Return the previous assigned backends, or a default backend if none was assigned.","title":"backends"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.conversations","text":"Return the previous assigned conversations, or an empty dictionary if none was assigned.","title":"conversations"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.facts","text":"Return the facts from the adapter.","title":"facts"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.heuristics","text":"Return the heuristics from the adapter.","title":"heuristics"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.inferencers","text":"Return the inferencers from the adapter.","title":"inferencers"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.name","text":"Return the previous assigned name, or a default name if none was assigned.","title":"name"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.slm","text":"Return the previous assigned SLM, or a default SLM if none was assigned.","title":"slm"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.track_conversations","text":"Return the previous assigned track_conversations, or a default value if none was assigned.","title":"track_conversations"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.custom_discuss","text":"Send user input to our SLM and return the reply, AND the actual user input. In the base implementation, the user_input is unchanged from what we are given. But derived classes can override this method to do things like: Add other context info to the user_input Query other models first and combine their replies to form a single user_input etc.","title":"custom_discuss()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.get_conversation","text":"Return the conversation with the given id. Instantiate a new conversation if none was found, and an id was given.","title":"get_conversation()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.load","text":"Loads the SSM from the specified directory.","title":"load()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.save","text":"Saves the SSM to the specified directory.","title":"save()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.update_conversation","text":"Update the conversation with the user_input and reply.","title":"update_conversation()"},{"location":"openssm/core/ssm/base_ssm_builder/","text":"BaseSSMBuilder Bases: AbstractSSMBuilder add_inferencer ( inferencer , knowledge_id ) Adds or creates an inferencer (e.g., ML models) to a specific knowledge source add_knowledge ( knowledge_source_uri , source_type = None ) Uploads a knowledge source (documents, text, files, etc.) create_ssm ( knowledge_ids , model_parameters = None ) Creates an SSM based on the provided knowledge sources and model parameters extract_structured_information ( knowledge_id ) Extracts structured information (facts, heuristics) from a specific knowledge source generate_training_data ( knowledge_id , prompt_parameters = None ) Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model train_slm ( model , training_data , fine_tuning_parameters = None ) Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"base_ssm_builder"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder","text":"Bases: AbstractSSMBuilder","title":"BaseSSMBuilder"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.add_inferencer","text":"Adds or creates an inferencer (e.g., ML models) to a specific knowledge source","title":"add_inferencer()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.create_ssm","text":"Creates an SSM based on the provided knowledge sources and model parameters","title":"create_ssm()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.extract_structured_information","text":"Extracts structured information (facts, heuristics) from a specific knowledge source","title":"extract_structured_information()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.generate_training_data","text":"Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model","title":"generate_training_data()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.train_slm","text":"Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"train_slm()"},{"location":"openssm/core/ssm/rag_ssm/","text":"RAGSSM Bases: BaseSSM custom_discuss ( user_input , conversation ) An SSM with a RAG backend will reason between its own SLM\u2019s knowledge and the knowledge of the RAG backend, before return the response. The process proceeds as follows: We first queries the RAG backend for a response. We then query the SLM for its response We combine the two responses into a single query to the SLM The SLM\u2019s response is then returned.","title":"rag_ssm"},{"location":"openssm/core/ssm/rag_ssm/#openssm.core.ssm.rag_ssm.RAGSSM","text":"Bases: BaseSSM","title":"RAGSSM"},{"location":"openssm/core/ssm/rag_ssm/#openssm.core.ssm.rag_ssm.RAGSSM.custom_discuss","text":"An SSM with a RAG backend will reason between its own SLM\u2019s knowledge and the knowledge of the RAG backend, before return the response. The process proceeds as follows: We first queries the RAG backend for a response. We then query the SLM for its response We combine the two responses into a single query to the SLM The SLM\u2019s response is then returned.","title":"custom_discuss()"},{"location":"openssm/deprecated/chatssm-vinh/ssms/semiconductor_ssm/EMPTY/","text":"This directory is (still) empty.","title":"semiconductor_ssm"},{"location":"openssm/deprecated/chatssm-vinh/static/images/EMPTY/","text":"This directory is (still) empty.","title":"images"},{"location":"openssm/industrial/interpretability/EMPTY/","text":"This directory is (still) empty.","title":"interpretability"},{"location":"openssm/industrial/monitoring/EMPTY/","text":"This directory is (still) empty.","title":"monitoring"},{"location":"openssm/industrial/security/audit/EMPTY/","text":"This directory is (still) empty.","title":"audit"},{"location":"openssm/industrial/security/best_practices/EMPTY/","text":"This directory is (still) empty.","title":"best_practices"},{"location":"openssm/integrations/huggingface/slm/","text":"This module contains the HuggingFaceBaseSLM class, and its subclasses, which are SLMs based on models from HugoingFace. The models may be served from HuggingFace's model hub, or a private internal server. Falcon7bSLM Bases: SLM Falcon7bSLM is a wrapper for the Falcon7b model, which may be hosted remotely. If hosted remotely, the model_url and model_server_token must be provided through the Config class. FALCON7B_MODEL_URL should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If not supported, set to \"NONE\" (or not set at all) SLM Bases: BaseSLM This class is the base class for all SLMs based on models from HuggingFace. The models may be served from HuggingFace's model hub, or a private internal server. model_url should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If not supported, set to \"NONE\" (or not set at all)","title":"slm"},{"location":"openssm/integrations/huggingface/slm/#openssm.integrations.huggingface.slm.Falcon7bSLM","text":"Bases: SLM Falcon7bSLM is a wrapper for the Falcon7b model, which may be hosted remotely. If hosted remotely, the model_url and model_server_token must be provided through the Config class. FALCON7B_MODEL_URL should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If not supported, set to \"NONE\" (or not set at all)","title":"Falcon7bSLM"},{"location":"openssm/integrations/huggingface/slm/#openssm.integrations.huggingface.slm.SLM","text":"Bases: BaseSLM This class is the base class for all SLMs based on models from HuggingFace. The models may be served from HuggingFace's model hub, or a private internal server. model_url should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If not supported, set to \"NONE\" (or not set at all)","title":"SLM"},{"location":"openssm/integrations/huggingface/ssm/","text":"","title":"ssm"},{"location":"openssm/integrations/lepton_ai/slm/","text":"","title":"slm"},{"location":"openssm/integrations/lepton_ai/ssm/","text":"","title":"ssm"},{"location":"openssm/integrations/llama_index/","text":"OpenSSM and LlamaIndex Integration This guide provides an overview and examples of how Small Specialist Models (SSMs, from the OpenSSM project) integrate with LlamaIndex. Overview SSMs are designed to be private, secure, domain-specific models (or AI agents) for industrial applications. LlamaIndex is a simple, flexible data framework for connecting custom data sources to LLMs. As such, there are two major integration patterns: SSMs comprising LlamaIndex in its backend, for data access, e.g., via retrieval-augmented generation. SSMs serving as data sources or data agents for LlamaIndex, e.g., for multi-agent sourcing and planning. When integrated, both bring unique benefits that greatly enhance their collective capabilities. SSMs can leverage LlamaIndex to access specific, contextually relevant data, enhancing their specialist capabilities. Conversely, SSMs, as data sources for LlamaIndex, contribute their nuanced domain knowledge to a broader data ecosystem. Additionally, this integration promotes efficiency and customization, thanks to LlamaIndex\u2019s flexibility in handling different data formats and SSMs\u2019 computational advantages (e.g., from domain-specific, fine-tuned, distilled language models). The relationship between SSMs and LlamaIndex enriches the the LlamaIndex ecosystem and while helping to improve the robustness and reliability of SSMs. Integration Examples Here are some examples to get you started. Basic Integration OpenSSM makes using LlamaIndex as simple as 3 lines of code: from openssm import LlamaIndexSSM # Instantiate a LlamaIndexSSM ssm = LlamaIndexSSM() ssm.read_directory('docs/ylecun') # Read the docs for the first time ssm.discuss(\"What is the main point made by Yann LeCun?\") # Interact with the SSM Persistence is just as straightforward: ssm.save('storage/ylecun') # Save the index to storage ssm.load('storage/ylecun') # Load the index from storage Domain-specific SSM In the example below, we put a domain-specific SSM (an SLM or small language model trained on data related to Yann LeCun\u2019s work) in front of LlamaIndex. from openssm import LlamaIndexSSM, FineTunedSLM slm = FineTunedSLM(...) # Instantiate a domain-specific SLM ssm = LlamaIndexSSM(slm=slm) # Instantiate a LlamaIndexSSM with the SLM ssm.read_directory('docs/ylecun') # Read the docs response = ssm.discuss(\"What is the main point made by Yann LeCun?\") The response from this ssm would be much richer and more informed about Yann LeCun\u2019s work than a generic SSM performing the same task. In all of the above examples, the SSM is using LlamaIndex as a Backend , as shown below. Advanced Use Cases with LlamaIndex\u2019s Data Agents LlamaIndex\u2019s Data Agents, with their ability to dynamically read, write, search, and modify data across diverse sources, are a game changer for complex and automated tasks. Here, we cover three primary use cases: Here, we cover three primary use cases: Context Retrieval An agent can retrieve context-specific data to inform responses. For example, in a financial setting: from openssm import LlamaIndexSSM, ContextRetrievalAgent context = \"\"\" XYZ company reported Q2 revenues of $4.5 billion, up 18% YoY. The rise is primarily due to a 32% growth in their cloud division. \"\"\" agent = ContextRetrievalAgent(context) ssm = LlamaIndexSSM(agents=[context_agent]) ssm.read_directory('docs/financial_reports') response = ssm.discuss(\"What is the current financial performance of XYZ company?\") This agent can retrieve and analyze data from relevant financial reports, taking into account the context of recently reported Q2 revenues of $4.5 billion, to provide an informed response. Function Retrieval In cases where the set of tools is extensive, the agent can retrieve the most relevant ones dynamically during query time. For example, in a data analysis setting: from openssm import LlamaIndexSSM, FunctionRetrievalAgent agent = FunctionRetrievalAgent('tools/data_tools') ssm = LlamaIndexSSM(agents=[tool_agent]) ssm.read_directory('docs/financial_reports') response = ssm.discuss(\"Perform a correlation analysis on the financial reports\") This allows the SSM to retrieve and apply the most suitable data analysis tool based on the request. Query Planning For more complex tasks, OpenSSM can be made capable of advanced query planning thanks to LlamaIndex. It could, for instance, plan and execute a series of queries to answer a question about a company\u2019s revenue growth over specific months. from openssm import LlamaIndexSSM, QueryPlanningAgent query_plan_tool = QueryPlanTool.from_defaults( query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march] ) agent = QueryPlanningAgent(tools=[query_tool_sept, query_tool_june, query_tool_march]) ssm = LlamaIndexSSM(agents=[agent]) ssm.read_directory('../tmp/docs/financial_reports') response = ssm.discuss(\"What was the revenue growth of XYZ company from March through September?\") This illustrates how an SSM with a Query Planning Agent can plan and execute a series of queries to answer a complex question accurately. Future Enhancements As we continue to enhance the integration between OpenSSM and LlamaIndex, here are a few promising directions: SSMs as agents for LlamaIndex : We are exploring ways to make SSMs available as agents for LlamaIndex, allowing for more complex interactions between SSMs and LlamaIndex. Expansion to More Domain Areas : We are planning to develop SSMs for more specific domains, such as healthcare, law, and finance, and integrate these with LlamaIndex. Advanced Data Agents : The development and inclusion of more advanced and specialized data agents is a key part of our roadmap. Inter-Agent Communication : We plan to introduce advanced inter-agent communication protocols, allowing for more complex interactions between SSMs. Agent Collaboration on Complex Tasks : Building on the inter-agent communication, we are also exploring ways for multiple SSMs to collaborate on more complex tasks. Summary This guide provides an introduction to integrating Small Specialist Models (SSMs) with LlamaIndex. The relationship between the two enhance the performance of individual SSMs, and significantly elevates the utility of the broader AI ecosystem. This is particularly needed for industrial companies, where the ability to leverage domain-specific knowledge is critical for success.","title":"LlamaIndex"},{"location":"openssm/integrations/llama_index/#openssm-and-llamaindex-integration","text":"This guide provides an overview and examples of how Small Specialist Models (SSMs, from the OpenSSM project) integrate with LlamaIndex.","title":"OpenSSM and LlamaIndex Integration"},{"location":"openssm/integrations/llama_index/#overview","text":"SSMs are designed to be private, secure, domain-specific models (or AI agents) for industrial applications. LlamaIndex is a simple, flexible data framework for connecting custom data sources to LLMs. As such, there are two major integration patterns: SSMs comprising LlamaIndex in its backend, for data access, e.g., via retrieval-augmented generation. SSMs serving as data sources or data agents for LlamaIndex, e.g., for multi-agent sourcing and planning. When integrated, both bring unique benefits that greatly enhance their collective capabilities. SSMs can leverage LlamaIndex to access specific, contextually relevant data, enhancing their specialist capabilities. Conversely, SSMs, as data sources for LlamaIndex, contribute their nuanced domain knowledge to a broader data ecosystem. Additionally, this integration promotes efficiency and customization, thanks to LlamaIndex\u2019s flexibility in handling different data formats and SSMs\u2019 computational advantages (e.g., from domain-specific, fine-tuned, distilled language models). The relationship between SSMs and LlamaIndex enriches the the LlamaIndex ecosystem and while helping to improve the robustness and reliability of SSMs.","title":"Overview"},{"location":"openssm/integrations/llama_index/#integration-examples","text":"Here are some examples to get you started.","title":"Integration Examples"},{"location":"openssm/integrations/llama_index/#basic-integration","text":"OpenSSM makes using LlamaIndex as simple as 3 lines of code: from openssm import LlamaIndexSSM # Instantiate a LlamaIndexSSM ssm = LlamaIndexSSM() ssm.read_directory('docs/ylecun') # Read the docs for the first time ssm.discuss(\"What is the main point made by Yann LeCun?\") # Interact with the SSM Persistence is just as straightforward: ssm.save('storage/ylecun') # Save the index to storage ssm.load('storage/ylecun') # Load the index from storage","title":"Basic Integration"},{"location":"openssm/integrations/llama_index/#domain-specific-ssm","text":"In the example below, we put a domain-specific SSM (an SLM or small language model trained on data related to Yann LeCun\u2019s work) in front of LlamaIndex. from openssm import LlamaIndexSSM, FineTunedSLM slm = FineTunedSLM(...) # Instantiate a domain-specific SLM ssm = LlamaIndexSSM(slm=slm) # Instantiate a LlamaIndexSSM with the SLM ssm.read_directory('docs/ylecun') # Read the docs response = ssm.discuss(\"What is the main point made by Yann LeCun?\") The response from this ssm would be much richer and more informed about Yann LeCun\u2019s work than a generic SSM performing the same task. In all of the above examples, the SSM is using LlamaIndex as a Backend , as shown below.","title":"Domain-specific SSM"},{"location":"openssm/integrations/llama_index/#advanced-use-cases-with-llamaindexs-data-agents","text":"LlamaIndex\u2019s Data Agents, with their ability to dynamically read, write, search, and modify data across diverse sources, are a game changer for complex and automated tasks. Here, we cover three primary use cases: Here, we cover three primary use cases:","title":"Advanced Use Cases with LlamaIndex\u2019s Data Agents"},{"location":"openssm/integrations/llama_index/#context-retrieval","text":"An agent can retrieve context-specific data to inform responses. For example, in a financial setting: from openssm import LlamaIndexSSM, ContextRetrievalAgent context = \"\"\" XYZ company reported Q2 revenues of $4.5 billion, up 18% YoY. The rise is primarily due to a 32% growth in their cloud division. \"\"\" agent = ContextRetrievalAgent(context) ssm = LlamaIndexSSM(agents=[context_agent]) ssm.read_directory('docs/financial_reports') response = ssm.discuss(\"What is the current financial performance of XYZ company?\") This agent can retrieve and analyze data from relevant financial reports, taking into account the context of recently reported Q2 revenues of $4.5 billion, to provide an informed response.","title":"Context Retrieval"},{"location":"openssm/integrations/llama_index/#function-retrieval","text":"In cases where the set of tools is extensive, the agent can retrieve the most relevant ones dynamically during query time. For example, in a data analysis setting: from openssm import LlamaIndexSSM, FunctionRetrievalAgent agent = FunctionRetrievalAgent('tools/data_tools') ssm = LlamaIndexSSM(agents=[tool_agent]) ssm.read_directory('docs/financial_reports') response = ssm.discuss(\"Perform a correlation analysis on the financial reports\") This allows the SSM to retrieve and apply the most suitable data analysis tool based on the request.","title":"Function Retrieval"},{"location":"openssm/integrations/llama_index/#query-planning","text":"For more complex tasks, OpenSSM can be made capable of advanced query planning thanks to LlamaIndex. It could, for instance, plan and execute a series of queries to answer a question about a company\u2019s revenue growth over specific months. from openssm import LlamaIndexSSM, QueryPlanningAgent query_plan_tool = QueryPlanTool.from_defaults( query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march] ) agent = QueryPlanningAgent(tools=[query_tool_sept, query_tool_june, query_tool_march]) ssm = LlamaIndexSSM(agents=[agent]) ssm.read_directory('../tmp/docs/financial_reports') response = ssm.discuss(\"What was the revenue growth of XYZ company from March through September?\") This illustrates how an SSM with a Query Planning Agent can plan and execute a series of queries to answer a complex question accurately.","title":"Query Planning"},{"location":"openssm/integrations/llama_index/#future-enhancements","text":"As we continue to enhance the integration between OpenSSM and LlamaIndex, here are a few promising directions: SSMs as agents for LlamaIndex : We are exploring ways to make SSMs available as agents for LlamaIndex, allowing for more complex interactions between SSMs and LlamaIndex. Expansion to More Domain Areas : We are planning to develop SSMs for more specific domains, such as healthcare, law, and finance, and integrate these with LlamaIndex. Advanced Data Agents : The development and inclusion of more advanced and specialized data agents is a key part of our roadmap. Inter-Agent Communication : We plan to introduce advanced inter-agent communication protocols, allowing for more complex interactions between SSMs. Agent Collaboration on Complex Tasks : Building on the inter-agent communication, we are also exploring ways for multiple SSMs to collaborate on more complex tasks.","title":"Future Enhancements"},{"location":"openssm/integrations/llama_index/#summary","text":"This guide provides an introduction to integrating Small Specialist Models (SSMs) with LlamaIndex. The relationship between the two enhance the performance of individual SSMs, and significantly elevates the utility of the broader AI ecosystem. This is particularly needed for industrial companies, where the ability to leverage domain-specific knowledge is critical for success.","title":"Summary"},{"location":"openssm/integrations/llama_index/backend/","text":"Backend dataclass Bases: AbstractRAGBackend __init__ ( relevance_threshold = 0.5 ) Initialize the backend. @param relevance_threshold: The relevance threshold for the MMR query engine. 0-1 (default: 0.5) The higher the threshold, the stricter the document relevance requirement. Increasing the threshold increases the relevance, but also decreases the chance of finding an answer. query ( user_input , conversation = None ) Query the index with the user input. Returns a tuple comprising (a) the response dicts and (b) the response object, if any.","title":"backend"},{"location":"openssm/integrations/llama_index/backend/#openssm.integrations.llama_index.backend.Backend","text":"Bases: AbstractRAGBackend","title":"Backend"},{"location":"openssm/integrations/llama_index/backend/#openssm.integrations.llama_index.backend.Backend.__init__","text":"Initialize the backend. @param relevance_threshold: The relevance threshold for the MMR query engine. 0-1 (default: 0.5) The higher the threshold, the stricter the document relevance requirement. Increasing the threshold increases the relevance, but also decreases the chance of finding an answer.","title":"__init__()"},{"location":"openssm/integrations/llama_index/backend/#openssm.integrations.llama_index.backend.Backend.query","text":"Query the index with the user input. Returns a tuple comprising (a) the response dicts and (b) the response object, if any.","title":"query()"},{"location":"openssm/integrations/llama_index/ssm/","text":"","title":"ssm"},{"location":"openssm/integrations/openai/slm/","text":"","title":"slm"},{"location":"openssm/integrations/openai/ssm/","text":"","title":"ssm"},{"location":"openssm/integrations/testing_tools/EMPTY/","text":"This directory is (still) empty.","title":"testing_tools"},{"location":"openssm/integrations/vectara/EMPTY/","text":"This directory is (still) empty.","title":"vectara"},{"location":"openssm/utils/config/","text":"Config This class is used to store config setings, as well as secrets, such as API keys, tokens, etc. By default, they come from documented environment variables. But the user can override them by setting them directly in the Config object. setenv ( var_name ) staticmethod Copy the value of a config variable to an environment variable. If the variable is not set, nothing is changed.","title":"config"},{"location":"openssm/utils/config/#openssm.utils.config.Config","text":"This class is used to store config setings, as well as secrets, such as API keys, tokens, etc. By default, they come from documented environment variables. But the user can override them by setting them directly in the Config object.","title":"Config"},{"location":"openssm/utils/config/#openssm.utils.config.Config.setenv","text":"Copy the value of a config variable to an environment variable. If the variable is not set, nothing is changed.","title":"setenv()"},{"location":"openssm/utils/logs/","text":"Logs do_log_entry ( * extra_args , log_level = logging . DEBUG ) staticmethod Decorator to log function entry. do_log_entry_and_exit ( * extra_args , the_logger = None , log_level = logging . DEBUG , log_entry = True , log_exit = True ) staticmethod Decorator to log function entry and exit. do_log_exit ( * extra_args , log_level = logging . DEBUG ) staticmethod Decorator to log function exit. get_logger ( name = None , log_level = logging . DEBUG ) staticmethod Gets a new/existing logger with the given name and log level","title":"logs"},{"location":"openssm/utils/logs/#openssm.utils.logs.Logs","text":"","title":"Logs"},{"location":"openssm/utils/logs/#openssm.utils.logs.Logs.do_log_entry","text":"Decorator to log function entry.","title":"do_log_entry()"},{"location":"openssm/utils/logs/#openssm.utils.logs.Logs.do_log_entry_and_exit","text":"Decorator to log function entry and exit.","title":"do_log_entry_and_exit()"},{"location":"openssm/utils/logs/#openssm.utils.logs.Logs.do_log_exit","text":"Decorator to log function exit.","title":"do_log_exit()"},{"location":"openssm/utils/logs/#openssm.utils.logs.Logs.get_logger","text":"Gets a new/existing logger with the given name and log level","title":"get_logger()"},{"location":"openssm/utils/utils/","text":"Utils canonicalize_discuss_result ( result ) staticmethod Make sure response is in the form of a dict, e.g., {\"role\": \"assistant\", \"content\": \"hello\"}. canonicalize_query_response ( response ) staticmethod Make sure response is in the form of a dict e.g., {\"response\": \"hello\", \"response_object\": } canonicalize_user_input ( user_input ) staticmethod Make sure user_input is in the form of a list of dicts, e.g., [{\"role\": \"user\", \"content\": \"hello\"}]. do_canonicalize_discuss_result ( func ) staticmethod Decorator to canonicalize SSM discuss result. do_canonicalize_query_response ( func ) staticmethod Decorator to canonicalize Backend query response. do_canonicalize_user_input ( param_name ) staticmethod Decorator to canonicalize SSM user input.","title":"utils"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils","text":"","title":"Utils"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.canonicalize_discuss_result","text":"Make sure response is in the form of a dict, e.g., {\"role\": \"assistant\", \"content\": \"hello\"}.","title":"canonicalize_discuss_result()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.canonicalize_query_response","text":"Make sure response is in the form of a dict e.g., {\"response\": \"hello\", \"response_object\": }","title":"canonicalize_query_response()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.canonicalize_user_input","text":"Make sure user_input is in the form of a list of dicts, e.g., [{\"role\": \"user\", \"content\": \"hello\"}].","title":"canonicalize_user_input()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.do_canonicalize_discuss_result","text":"Decorator to canonicalize SSM discuss result.","title":"do_canonicalize_discuss_result()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.do_canonicalize_query_response","text":"Decorator to canonicalize Backend query response.","title":"do_canonicalize_query_response()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.do_canonicalize_user_input","text":"Decorator to canonicalize SSM user input.","title":"do_canonicalize_user_input()"},{"location":"support/","text":"Resources for model support and maintenance","title":"Support"},{"location":"support/#resources-for-model-support-and-maintenance","text":"","title":"Resources for model support and maintenance"},{"location":"support/FAQ/","text":"","title":"FAQ"},{"location":"support/troubleshooting_guides/","text":"","title":"Troubleshoot"}]}