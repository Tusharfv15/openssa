{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenSSM \u2013 \u201cSmall Specialist Models\u201d for Industrial AI See full documentation at aitomatic.github.io/openssm/ . OpenSSM (pronounced open-ess-ess-em ) is an open-source framework for Small Specialist Models (SSMs), which are key to enhancing trust, reliability, and safety in Industrial-AI applications. Harnessing the power of domain expertise, SSMs operate either alone or in \"teams\". They collaborate with other SSMs, planners, and sensors/actuators to deliver real-world problem-solving capabilities. Unlike Large Language Models (LLMs), which are computationally intensive and generalized, SSMs are lean, efficient, and designed specifically for individual domains. This focus makes them an optimal choice for businesses, SMEs, researchers, and developers seeking specialized and robust AI solutions for industrial applications. A prime deployment scenario for SSMs is within the aiCALM (Collaborative Augmented Large Models) architecture. aiCALM represents a cohesive assembly of AI components tailored for sophisticated problem-solving capabilities. Within this framework, SSMs work with General Management Models (GMMs) and other components to solve complex, domain-specific, and industrial problems. Why SSM? The trend towards specialization in AI models is a clear trajectory seen by many in the field. Specialization is crucial for quality .. not general purpose Al models \u2013 Eric Schmidt, Schmidt Foundation .. small models .. for a specific task that are good \u2013 Matei Zaharia, Databricks .. small agents working together .. specific and best in their tasks \u2013 Harrison Chase, Langchain .. small but highly capable expert models \u2013 Andrej Karpathy, OpenAI .. small models are .. a massive paradigm shift .. about deploying AI models at scale \u2013 Rob Toews, Radical Ventures As predicted by Eric Schmidt and others, we will see \u201ca rich ecosystem to emerge [of] high-value, specialized AI systems.\u201d SSMs are the central part in the architecture of these systems. What OpenSSM Offers OpenSSM fills this gap directly, with the following benefits to the community, developers, and businesses: Industrial Focus: SSMs are developed with a specific emphasis on industrial applications, addressing the unique requirements of trustworthiness, safety, reliability, and scalability inherent to this sector. Fast, Cost-Effective & Easy to Use: SSMs are 100-1000x faster and more efficient than LLMs, making them accessible and cost-effective particularly for industrial usage where time and resources are critical factors. Easy Knowledge Capture: OpenSSM has easy-to-use tools for capturing domain knowledge in diverse forms: books, operaring manuals, databases, knowledge graphs, text files, and code. Powerful Operations on Captured Knowledge: OpenSSM enables both knowledge query and inferencing/predictive capabilities based on the domain-specific knowledge. Collaborative Problem-Solving : SSMs are designed to work in problem-solving \"teams\". Multi-SSM collaboration is a first-class design feature, not an afterthought. Reliable Domain Expertise: Each SSM has expertise in a particular field or equipment, offering precise and specialized knowledge, thereby enhancing trustworthiness, reliability, and safety for Industrial-AI applications. With self-reasoning, causal reasoning, and retrieval-based knowledge, SSMs provide a trustable source of domain expertise. Vendor Independence: OpenSSM allows everyone to build, train, and deploy their own domain-expert AI models, offering freedom from vendor lock-in and security concerns. Composable Expertise : SSMs are fully composable, making it easy to combine domain expertise. Target Audience Our primary audience includes: Businesses and SMEs wishing to leverage AI in their specific industrial context without relying on extensive computational resources or large vendor solutions. AI researchers and developers keen on creating more efficient, robust, and domain-specific AI models for industrial applications. Open-source contributors believing in democratizing industrial AI and eager to contribute to a community-driven project focused on building and sharing specialized AI models. Industries with specific domain problems that can be tackled more effectively by a specialist AI model, enhancing the reliability and trustworthiness of AI solutions in an industrial setting. SSM Architecture At a high level, SSMs comprise a front-end Small Language Model (SLM), an adapter layer in the middle, and a wide range of back-end domain-knowledge sources. The SLM itself is a small, efficient, language model, which may be domain-specific or not, and may have been distilled from a larger model. Thus, domain knowledge may come from either, or both, the SLM and the backends. The above diagram illustrates the high-level architecture of an SSM, which comprises three main components: Small Language Model (SLM): This forms the communication frontend of an SSM. Adapters (e.g., LlamaIndex): These provide the interface between the SLM and the domain-knowledge backends. Domain-Knowledge Backends: These include text files, documents, PDFs, databases, code, knowledge graphs, models, other SSMs, etc. SSMs communicate in both unstructured (natural language) and structured APIs, catering to a variety of real-world industrial systems. The composable nature of SSMs allows for easy combination of domain-knowledge sources from multiple models. Getting Started See our Getting Started Guide for more information. Roadmap Play with SSMs in a hosted SSM sandbox, uploading your own domain knowledge Create SSMs in your own development environment, and integrate SSMs into your own AI apps Capture domain knowledge in various forms into your SSMs Train SLMs via distillation of LLMs, teacher/student approaches, etc. Apply SSMs in collaborative problem-solving AI systems Community Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions . Contribute OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. License OpenSSM is released under the Apache 2.0 License .","title":"Home"},{"location":"#openssm-small-specialist-models-for-industrial-ai","text":"See full documentation at aitomatic.github.io/openssm/ . OpenSSM (pronounced open-ess-ess-em ) is an open-source framework for Small Specialist Models (SSMs), which are key to enhancing trust, reliability, and safety in Industrial-AI applications. Harnessing the power of domain expertise, SSMs operate either alone or in \"teams\". They collaborate with other SSMs, planners, and sensors/actuators to deliver real-world problem-solving capabilities. Unlike Large Language Models (LLMs), which are computationally intensive and generalized, SSMs are lean, efficient, and designed specifically for individual domains. This focus makes them an optimal choice for businesses, SMEs, researchers, and developers seeking specialized and robust AI solutions for industrial applications. A prime deployment scenario for SSMs is within the aiCALM (Collaborative Augmented Large Models) architecture. aiCALM represents a cohesive assembly of AI components tailored for sophisticated problem-solving capabilities. Within this framework, SSMs work with General Management Models (GMMs) and other components to solve complex, domain-specific, and industrial problems.","title":"OpenSSM \u2013 \u201cSmall Specialist Models\u201d for Industrial AI"},{"location":"#why-ssm","text":"The trend towards specialization in AI models is a clear trajectory seen by many in the field. Specialization is crucial for quality .. not general purpose Al models \u2013 Eric Schmidt, Schmidt Foundation .. small models .. for a specific task that are good \u2013 Matei Zaharia, Databricks .. small agents working together .. specific and best in their tasks \u2013 Harrison Chase, Langchain .. small but highly capable expert models \u2013 Andrej Karpathy, OpenAI .. small models are .. a massive paradigm shift .. about deploying AI models at scale \u2013 Rob Toews, Radical Ventures As predicted by Eric Schmidt and others, we will see \u201ca rich ecosystem to emerge [of] high-value, specialized AI systems.\u201d SSMs are the central part in the architecture of these systems.","title":"Why SSM?"},{"location":"#what-openssm-offers","text":"OpenSSM fills this gap directly, with the following benefits to the community, developers, and businesses: Industrial Focus: SSMs are developed with a specific emphasis on industrial applications, addressing the unique requirements of trustworthiness, safety, reliability, and scalability inherent to this sector. Fast, Cost-Effective & Easy to Use: SSMs are 100-1000x faster and more efficient than LLMs, making them accessible and cost-effective particularly for industrial usage where time and resources are critical factors. Easy Knowledge Capture: OpenSSM has easy-to-use tools for capturing domain knowledge in diverse forms: books, operaring manuals, databases, knowledge graphs, text files, and code. Powerful Operations on Captured Knowledge: OpenSSM enables both knowledge query and inferencing/predictive capabilities based on the domain-specific knowledge. Collaborative Problem-Solving : SSMs are designed to work in problem-solving \"teams\". Multi-SSM collaboration is a first-class design feature, not an afterthought. Reliable Domain Expertise: Each SSM has expertise in a particular field or equipment, offering precise and specialized knowledge, thereby enhancing trustworthiness, reliability, and safety for Industrial-AI applications. With self-reasoning, causal reasoning, and retrieval-based knowledge, SSMs provide a trustable source of domain expertise. Vendor Independence: OpenSSM allows everyone to build, train, and deploy their own domain-expert AI models, offering freedom from vendor lock-in and security concerns. Composable Expertise : SSMs are fully composable, making it easy to combine domain expertise.","title":"What OpenSSM Offers"},{"location":"#target-audience","text":"Our primary audience includes: Businesses and SMEs wishing to leverage AI in their specific industrial context without relying on extensive computational resources or large vendor solutions. AI researchers and developers keen on creating more efficient, robust, and domain-specific AI models for industrial applications. Open-source contributors believing in democratizing industrial AI and eager to contribute to a community-driven project focused on building and sharing specialized AI models. Industries with specific domain problems that can be tackled more effectively by a specialist AI model, enhancing the reliability and trustworthiness of AI solutions in an industrial setting.","title":"Target Audience"},{"location":"#ssm-architecture","text":"At a high level, SSMs comprise a front-end Small Language Model (SLM), an adapter layer in the middle, and a wide range of back-end domain-knowledge sources. The SLM itself is a small, efficient, language model, which may be domain-specific or not, and may have been distilled from a larger model. Thus, domain knowledge may come from either, or both, the SLM and the backends. The above diagram illustrates the high-level architecture of an SSM, which comprises three main components: Small Language Model (SLM): This forms the communication frontend of an SSM. Adapters (e.g., LlamaIndex): These provide the interface between the SLM and the domain-knowledge backends. Domain-Knowledge Backends: These include text files, documents, PDFs, databases, code, knowledge graphs, models, other SSMs, etc. SSMs communicate in both unstructured (natural language) and structured APIs, catering to a variety of real-world industrial systems. The composable nature of SSMs allows for easy combination of domain-knowledge sources from multiple models.","title":"SSM Architecture"},{"location":"#getting-started","text":"See our Getting Started Guide for more information.","title":"Getting Started"},{"location":"#roadmap","text":"Play with SSMs in a hosted SSM sandbox, uploading your own domain knowledge Create SSMs in your own development environment, and integrate SSMs into your own AI apps Capture domain knowledge in various forms into your SSMs Train SLMs via distillation of LLMs, teacher/student approaches, etc. Apply SSMs in collaborative problem-solving AI systems","title":"Roadmap"},{"location":"#community","text":"Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions .","title":"Community"},{"location":"#contribute","text":"OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details.","title":"Contribute"},{"location":"#license","text":"OpenSSM is released under the Apache 2.0 License .","title":"License"},{"location":"GETTING_STARTED/","text":"Getting Started with OpenSSM Who Are You? An end-user of OpenSSM-based applications A developer of applications or services using OpenSSM An aspiring contributor to OpenSSM A committer to OpenSSM Getting Started as an End-User Getting Started as a Developer See some example user programs in the examples directory. For example, to run the chatssm example, do: % cd examples/chatssm % make clean % make Common make targets for OpenSSM developers See MAKEFILE for more details. % make clean % make build % make rebuild % make test % make poetry-init % make poetry-install % make install # local installation of openssm % make pypi-auth # only for maintainers % make publish # only for maintainers Getting Started as an Aspiring Contributor OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. You can begin contributing to the OpenSSM project in the contrib/ directory. Getting Started as a Committer You already know what to do. Community Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions . License OpenSSM is released under the Apache 2.0 License . Links MAKEFILE","title":"Getting Started"},{"location":"GETTING_STARTED/#getting-started-with-openssm","text":"","title":"Getting Started with OpenSSM"},{"location":"GETTING_STARTED/#who-are-you","text":"An end-user of OpenSSM-based applications A developer of applications or services using OpenSSM An aspiring contributor to OpenSSM A committer to OpenSSM","title":"Who Are You?"},{"location":"GETTING_STARTED/#getting-started-as-an-end-user","text":"","title":"Getting Started as an End-User"},{"location":"GETTING_STARTED/#getting-started-as-a-developer","text":"See some example user programs in the examples directory. For example, to run the chatssm example, do: % cd examples/chatssm % make clean % make","title":"Getting Started as a Developer"},{"location":"GETTING_STARTED/#common-make-targets-for-openssm-developers","text":"See MAKEFILE for more details. % make clean % make build % make rebuild % make test % make poetry-init % make poetry-install % make install # local installation of openssm % make pypi-auth # only for maintainers % make publish # only for maintainers","title":"Common make targets for OpenSSM developers"},{"location":"GETTING_STARTED/#getting-started-as-an-aspiring-contributor","text":"OpenSSM is a community-driven initiative, and we warmly welcome contributions. Whether it's enhancing existing models, creating new SSMs for different industrial domains, or improving our documentation, every contribution counts. See our Contribution Guide for more details. You can begin contributing to the OpenSSM project in the contrib/ directory.","title":"Getting Started as an Aspiring Contributor"},{"location":"GETTING_STARTED/#getting-started-as-a-committer","text":"You already know what to do.","title":"Getting Started as a Committer"},{"location":"GETTING_STARTED/#community","text":"Join our vibrant community of AI enthusiasts, researchers, developers, and businesses who are democratizing industrial AI through SSMs. Participate in the discussions, share your ideas, or ask for help on our Community Discussions .","title":"Community"},{"location":"GETTING_STARTED/#license","text":"OpenSSM is released under the Apache 2.0 License .","title":"License"},{"location":"GETTING_STARTED/#links","text":"MAKEFILE","title":"Links"},{"location":"LICENSE/","text":"OpenSSM Project License OpenSSM is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project's files except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"LICENSE/#openssm-project-license","text":"OpenSSM is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project's files except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"OpenSSM Project License"},{"location":"PROJECT_PHILOSOPHY/","text":"OpenSSM Project Philosophy At OpenSSM, we believe in the democratization of AI. Our goal is to create an ecosystem where anyone, regardless of their resources, can have access to efficient and domain-specific AI solutions. We envision a future where AI is not only accessible but also robust, reliable, and trustworthy. Our project is guided by the following principles: Collaboration: We aim to foster an environment of collaboration where multiple models can work together to solve complex problems. Empowerment: We strive to empower enterprises, SMEs, and individuals to build, train, and deploy their own AI models. Inclusivity: We are committed to creating a project that welcomes and includes contributions from everyone, regardless of their background, expertise, or resources. Transparency: We believe in open-source and the power of shared knowledge. Our code, our models, and our development processes are transparent and open to all. Excellence: We continuously strive for the highest standards in our models, ensuring they are efficient, reliable, and precise in their domain-specific knowledge. Our community is our greatest strength, and we are committed to nurturing it with these values in mind.","title":"Project Philosophy"},{"location":"PROJECT_PHILOSOPHY/#openssm-project-philosophy","text":"At OpenSSM, we believe in the democratization of AI. Our goal is to create an ecosystem where anyone, regardless of their resources, can have access to efficient and domain-specific AI solutions. We envision a future where AI is not only accessible but also robust, reliable, and trustworthy. Our project is guided by the following principles: Collaboration: We aim to foster an environment of collaboration where multiple models can work together to solve complex problems. Empowerment: We strive to empower enterprises, SMEs, and individuals to build, train, and deploy their own AI models. Inclusivity: We are committed to creating a project that welcomes and includes contributions from everyone, regardless of their background, expertise, or resources. Transparency: We believe in open-source and the power of shared knowledge. Our code, our models, and our development processes are transparent and open to all. Excellence: We continuously strive for the highest standards in our models, ensuring they are efficient, reliable, and precise in their domain-specific knowledge. Our community is our greatest strength, and we are committed to nurturing it with these values in mind.","title":"OpenSSM Project Philosophy"},{"location":"community/CODE_OF_CONDUCT/","text":"Code of Conduct This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in OpenSSM to help us create safe and positive experiences for everyone. Expected Behavior The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence, or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist, or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Consequences of Unacceptable Behavior Unacceptable behavior from any community member will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning. Reporting Guidelines If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. Addressing Grievances If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify the project team with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Scope We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business.","title":"Code of Conduct"},{"location":"community/CODE_OF_CONDUCT/#code-of-conduct","text":"This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in OpenSSM to help us create safe and positive experiences for everyone.","title":"Code of Conduct"},{"location":"community/CODE_OF_CONDUCT/#expected-behavior","text":"The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech.","title":"Expected Behavior"},{"location":"community/CODE_OF_CONDUCT/#unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence, or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist, or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability.","title":"Unacceptable Behavior"},{"location":"community/CODE_OF_CONDUCT/#consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any community member will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning.","title":"Consequences of Unacceptable Behavior"},{"location":"community/CODE_OF_CONDUCT/#reporting-guidelines","text":"If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible.","title":"Reporting Guidelines"},{"location":"community/CODE_OF_CONDUCT/#addressing-grievances","text":"If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify the project team with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies.","title":"Addressing Grievances"},{"location":"community/CODE_OF_CONDUCT/#scope","text":"We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business.","title":"Scope"},{"location":"community/CONTRIBUTING/","text":"Contributing to OpenSSM Thanks for your interest in contributing to OpenSSM! This document provides guidelines for contributing to the project. Please read these guidelines before submitting a contribution. Code of Conduct All contributors must abide by the Code of Conduct . Please read it before contributing. How to Contribute Find an issue to work on: Look at the list of open issues in the OpenSSM repository. Pick one that interests you and that no one else is working on. Fork the repository and create a branch: If you're not a project maintainer, you'll need to create a fork of the repository and create a branch on your fork where you can make your changes. Submit a pull request: After you've made your changes, submit a pull request to merge your branch into the main OpenSSM repository. Be sure to link the issue you're addressing in your pull request. Please ensure your contribution meets the following guidelines: Code contributions must be compatible with the project's license. We follow PEP8 for Python style guidelines. Include tests when adding new features. Contributions will not be accepted without them. Document new code based on the documentation style guide. Make sure all tests pass on your machine before you submit a pull request. Thank you for your contributions!","title":"Contributing"},{"location":"community/CONTRIBUTING/#contributing-to-openssm","text":"Thanks for your interest in contributing to OpenSSM! This document provides guidelines for contributing to the project. Please read these guidelines before submitting a contribution.","title":"Contributing to OpenSSM"},{"location":"community/CONTRIBUTING/#code-of-conduct","text":"All contributors must abide by the Code of Conduct . Please read it before contributing.","title":"Code of Conduct"},{"location":"community/CONTRIBUTING/#how-to-contribute","text":"Find an issue to work on: Look at the list of open issues in the OpenSSM repository. Pick one that interests you and that no one else is working on. Fork the repository and create a branch: If you're not a project maintainer, you'll need to create a fork of the repository and create a branch on your fork where you can make your changes. Submit a pull request: After you've made your changes, submit a pull request to merge your branch into the main OpenSSM repository. Be sure to link the issue you're addressing in your pull request. Please ensure your contribution meets the following guidelines: Code contributions must be compatible with the project's license. We follow PEP8 for Python style guidelines. Include tests when adding new features. Contributions will not be accepted without them. Document new code based on the documentation style guide. Make sure all tests pass on your machine before you submit a pull request. Thank you for your contributions!","title":"How to Contribute"},{"location":"dev/design_principles/","text":"OpenSSM Design Principles Specialization Over Generalization: Our models are designed to be domain-specific to provide precise solutions to specific problems, rather than providing generalized solutions. Efficiency and Speed: We aim for our models to be faster and more efficient than large language models, making AI more accessible and cost-effective. Trustworthiness and Reliability: As a foundation of industrial AI, our models are developed with an emphasis on robustness, reliability, and scalability. Collaborative Approach: We believe in the power of combined intelligence. Our models can be deployed together to solve complex problems. Community-driven: Our models are developed by the community, for the community. We welcome contributions from everyone, regardless of their background or expertise.","title":"Design Principles"},{"location":"dev/design_principles/#openssm-design-principles","text":"Specialization Over Generalization: Our models are designed to be domain-specific to provide precise solutions to specific problems, rather than providing generalized solutions. Efficiency and Speed: We aim for our models to be faster and more efficient than large language models, making AI more accessible and cost-effective. Trustworthiness and Reliability: As a foundation of industrial AI, our models are developed with an emphasis on robustness, reliability, and scalability. Collaborative Approach: We believe in the power of combined intelligence. Our models can be deployed together to solve complex problems. Community-driven: Our models are developed by the community, for the community. We welcome contributions from everyone, regardless of their background or expertise.","title":"OpenSSM Design Principles"},{"location":"dev/howtos/","text":"Helpful How-Tos Observability OpenSSM has built-in observability and tracing. Logging Users of OpenSSM should create their own loggers: import logging from OpenSSM import Logging logger = Logging.get_logger(app_name, logging.INFO) logger.warn(\"xyz = %s\", xyz) If you are an OpenSSM contributor, you may use the default logger: from openssm import logger logger.warn(\"xyz = %s\", xyz) Automatic function logging There are some useful decorators for automatically logging function entry and exit. from openssm import Logging @Logging.do_log_entry_and_exit # upon both entry and exit def func(param1, param2): @Logging.do_log_entry # only upon entry @Logging.do_log_exit # only upon exit The above will automatically log function entry with its parameters, and function exit with its return value. If you want to use your own logger with its own name, use from openssm import Logging, logger logger = Logging.get_logger(app_name, logging.INFO) @Logging.do_log_entry_and_exit(logger=logger) def func(param1, param2): Sometimes it is useful to be able to specify additional parameters to be logged: @Logging.do_log_entry_and_exit({'request': request})","title":"Other HowTos"},{"location":"dev/howtos/#helpful-how-tos","text":"","title":"Helpful How-Tos"},{"location":"dev/howtos/#observability","text":"OpenSSM has built-in observability and tracing.","title":"Observability"},{"location":"dev/howtos/#logging","text":"Users of OpenSSM should create their own loggers: import logging from OpenSSM import Logging logger = Logging.get_logger(app_name, logging.INFO) logger.warn(\"xyz = %s\", xyz) If you are an OpenSSM contributor, you may use the default logger: from openssm import logger logger.warn(\"xyz = %s\", xyz)","title":"Logging"},{"location":"dev/howtos/#automatic-function-logging","text":"There are some useful decorators for automatically logging function entry and exit. from openssm import Logging @Logging.do_log_entry_and_exit # upon both entry and exit def func(param1, param2): @Logging.do_log_entry # only upon entry @Logging.do_log_exit # only upon exit The above will automatically log function entry with its parameters, and function exit with its return value. If you want to use your own logger with its own name, use from openssm import Logging, logger logger = Logging.get_logger(app_name, logging.INFO) @Logging.do_log_entry_and_exit(logger=logger) def func(param1, param2): Sometimes it is useful to be able to specify additional parameters to be logged: @Logging.do_log_entry_and_exit({'request': request})","title":"Automatic function logging"},{"location":"dev/makefile_info/","text":"Makefile guide We use Makefiles extensively to help make the developer\u2019s life simpler and more efficient. Here are the key targets for the top-level Makefile . dev-setup : run this first to set up your dev environment. test : perform testing on both Python and JS code found. test-console : same as test , but also show all output on the console. lint : run pylint and eslint on the code base. pre-commit : perform both linting and testing prior to commits, or at least pull requests. build : build the library (using poetry). install : build and perform a pip install from the local .whl outputs. clean : remove all the start from a clean slate. publish : publish the .whl to Pypi (for pip install support). pypi-auth : convenient target to set up your Pypi auth token prior to publishing docs-build : build web-based documentation docs-deploy : deploy web-based documentation to GitHub, e.g., aitomatic.github.io/openssm Miscellaneous: internal use or sub-targets Links GETTING STARTED","title":"Using Makefile"},{"location":"dev/makefile_info/#makefile-guide","text":"We use Makefiles extensively to help make the developer\u2019s life simpler and more efficient. Here are the key targets for the top-level Makefile . dev-setup : run this first to set up your dev environment. test : perform testing on both Python and JS code found. test-console : same as test , but also show all output on the console. lint : run pylint and eslint on the code base. pre-commit : perform both linting and testing prior to commits, or at least pull requests. build : build the library (using poetry). install : build and perform a pip install from the local .whl outputs. clean : remove all the start from a clean slate. publish : publish the .whl to Pypi (for pip install support). pypi-auth : convenient target to set up your Pypi auth token prior to publishing docs-build : build web-based documentation docs-deploy : deploy web-based documentation to GitHub, e.g., aitomatic.github.io/openssm Miscellaneous: internal use or sub-targets","title":"Makefile guide"},{"location":"dev/makefile_info/#links","text":"GETTING STARTED","title":"Links"},{"location":"diagrams/","text":"Design Diagrams ssm.drawio ssm-class-diagram.drawio.png ssm-composability.drawio.png ssm-full-industrial-use-case.drawio.png ssm-industrial-use-case.drawio.png ssm-key-components.drawio.png ssm-llama-index-integration.drawio.png","title":"Diagrams"},{"location":"diagrams/#design-diagrams","text":"ssm.drawio ssm-class-diagram.drawio.png ssm-composability.drawio.png ssm-full-industrial-use-case.drawio.png ssm-industrial-use-case.drawio.png ssm-key-components.drawio.png ssm-llama-index-integration.drawio.png","title":"Design Diagrams"},{"location":"integrations/lepton_ai/","text":"Lepton.AI Integration Lepton.AI is a developer-centric platform to build, fine-tune, and deploy large models. With OpenSSM, you can create SSMs by calling the Lepton pipeline with just a few lines of code. from openssm import BaseSSM, LeptonSLMFactory ssm = BaseSSM(slm=LeptonSLMFactory.create()) response = ssm.discuss(conversation_id, \"what is abc?\") Integration Architecture In the OpenSSM context, Lepton helps finetune and distill the SLM (small language model) that front-ends an SSM. Roadmap","title":"Lepton.AI"},{"location":"integrations/lepton_ai/#leptonai-integration","text":"Lepton.AI is a developer-centric platform to build, fine-tune, and deploy large models. With OpenSSM, you can create SSMs by calling the Lepton pipeline with just a few lines of code. from openssm import BaseSSM, LeptonSLMFactory ssm = BaseSSM(slm=LeptonSLMFactory.create()) response = ssm.discuss(conversation_id, \"what is abc?\")","title":"Lepton.AI Integration"},{"location":"integrations/lepton_ai/#integration-architecture","text":"In the OpenSSM context, Lepton helps finetune and distill the SLM (small language model) that front-ends an SSM.","title":"Integration Architecture"},{"location":"integrations/lepton_ai/#roadmap","text":"","title":"Roadmap"},{"location":"integrations/llama_index/","text":"LlamaIndex Integration LlamaIndex is a popular open-source data framework that helps you connect your large language models (LLMs) to your data so you can build powerful LLM applications. With OpenSSM, you can simply use LlamaIndexSSM with just a few lines of code. from openssm import LlamaIndexSSM ssm = LlamaIndexSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\") Integration Architecture In the OpenSSM context, LlamaIndex is treated as a backend, as shown below. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the LlamaIndex backend. Roadmap","title":"LlamaIndex"},{"location":"integrations/llama_index/#llamaindex-integration","text":"LlamaIndex is a popular open-source data framework that helps you connect your large language models (LLMs) to your data so you can build powerful LLM applications. With OpenSSM, you can simply use LlamaIndexSSM with just a few lines of code. from openssm import LlamaIndexSSM ssm = LlamaIndexSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\")","title":"LlamaIndex Integration"},{"location":"integrations/llama_index/#integration-architecture","text":"In the OpenSSM context, LlamaIndex is treated as a backend, as shown below. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the LlamaIndex backend.","title":"Integration Architecture"},{"location":"integrations/llama_index/#roadmap","text":"","title":"Roadmap"},{"location":"integrations/vectara/","text":"Vectara Integration Vectara is a developer-first API platform for easily building conversational search experiences that feature best-in-class Retrieval, Summarization, and \u201cGrounded Generation\u201d that all but eliminates hallucinations. With OpenSSM, you can simply use Vectara with just a few lines of code. from openssm import VectaraSSM ssm = VectaraSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\") Integration Architecture In the OpenSSM context, Vectara is treated as a backend, as shown below.. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the Vectara backend. Roadmap","title":"Vectara"},{"location":"integrations/vectara/#vectara-integration","text":"Vectara is a developer-first API platform for easily building conversational search experiences that feature best-in-class Retrieval, Summarization, and \u201cGrounded Generation\u201d that all but eliminates hallucinations. With OpenSSM, you can simply use Vectara with just a few lines of code. from openssm import VectaraSSM ssm = VectaraSSM() ssm.read_directory(\"path/to/directory\") response = ssm.discuss(conversation_id, \"what is xyz?\")","title":"Vectara Integration"},{"location":"integrations/vectara/#integration-architecture","text":"In the OpenSSM context, Vectara is treated as a backend, as shown below.. LlamaIndexSSM is simply an SSM with a passthrough (dummy) SLM that sends user queries directory to the Vectara backend.","title":"Integration Architecture"},{"location":"integrations/vectara/#roadmap","text":"","title":"Roadmap"},{"location":"openssm/capture/EMPTY/","text":"This directory is (still) empty.","title":"capture"},{"location":"openssm/composer/EMPTY/","text":"This directory is (still) empty.","title":"composer"},{"location":"openssm/contrib/apps/EMPTY/","text":"This directory is (still) empty.","title":"apps"},{"location":"openssm/contrib/ssms/industrial_boilers_ssm/EMPTY/","text":"This directory is (still) empty.","title":"industrial_boilers_ssm"},{"location":"openssm/contrib/ssms/japan_fish_kcp_ssm/EMPTY/","text":"This directory is (still) empty.","title":"japan_fish_kcp_ssm"},{"location":"openssm/contrib/ssms/mri_operator_ssm/EMPTY/","text":"This directory is (still) empty.","title":"mri_operator_ssm"},{"location":"openssm/contrib/ssms/semiconductor_ssm/EMPTY/","text":"This directory is (still) empty.","title":"semiconductor_ssm"},{"location":"openssm/core/adapter/abstract_adapter/","text":"AbstractAdapter Bases: ABC The AbstractAdapter serves as the base for all concrete Adapter classes. It provides an interface for interaction between the Small Language Model (SLM) and the Backend. add_backend ( backend ) abstractmethod Adds a backend to our adapter get_backends () abstractmethod Returns our backends list_facts () abstractmethod Lists all known facts. list_heuristics () abstractmethod Lists all known heuristics. list_inferencers () abstractmethod Lists all known inferencers. query ( conversation_id , user_input ) abstractmethod Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response. select_facts ( criteria ) abstractmethod Selects or searches for facts based on provided criteria. select_heuristics ( criteria ) abstractmethod Selects or searches for heuristics based on provided criteria. select_inferencers ( criteria ) abstractmethod Selects or searches for inferencers based on provided criteria. set_backends ( backends ) abstractmethod Sets our backends","title":"abstract_adapter"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter","text":"Bases: ABC The AbstractAdapter serves as the base for all concrete Adapter classes. It provides an interface for interaction between the Small Language Model (SLM) and the Backend.","title":"AbstractAdapter"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.add_backend","text":"Adds a backend to our adapter","title":"add_backend()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.get_backends","text":"Returns our backends","title":"get_backends()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.list_facts","text":"Lists all known facts.","title":"list_facts()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.list_heuristics","text":"Lists all known heuristics.","title":"list_heuristics()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.list_inferencers","text":"Lists all known inferencers.","title":"list_inferencers()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.query","text":"Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response.","title":"query()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_facts","text":"Selects or searches for facts based on provided criteria.","title":"select_facts()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_heuristics","text":"Selects or searches for heuristics based on provided criteria.","title":"select_heuristics()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.select_inferencers","text":"Selects or searches for inferencers based on provided criteria.","title":"select_inferencers()"},{"location":"openssm/core/adapter/abstract_adapter/#openssm.core.adapter.abstract_adapter.AbstractAdapter.set_backends","text":"Sets our backends","title":"set_backends()"},{"location":"openssm/core/adapter/base_adapter/","text":"BaseAdapter Bases: AbstractAdapter Base adapter class for SSMs. add_backend ( backend ) Add a backend to the list of backends. add_fact ( fact ) Idiom: add a fact to the first backend we have. enumerate_backends ( lambda_function ) Enumerate backends and apply lambda function to each backend. get_backends () Side effect: if no backends are set, a default TextBackend is created. list_facts () List facts from all backends. list_heuristics () List heuristics from all backends. list_inferencers () List inferencers from all backends. query ( conversation_id , user_input ) Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response. select_facts ( criteria ) Select facts from all backends. select_heuristics ( criteria ) Select heuristics from all backends. select_inferencers ( criteria ) Select inferencers from all backends. set_backends ( backends ) Set the list of backends.","title":"base_adapter"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter","text":"Bases: AbstractAdapter Base adapter class for SSMs.","title":"BaseAdapter"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.add_backend","text":"Add a backend to the list of backends.","title":"add_backend()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.add_fact","text":"Idiom: add a fact to the first backend we have.","title":"add_fact()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.enumerate_backends","text":"Enumerate backends and apply lambda function to each backend.","title":"enumerate_backends()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.get_backends","text":"Side effect: if no backends are set, a default TextBackend is created.","title":"get_backends()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.list_facts","text":"List facts from all backends.","title":"list_facts()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.list_heuristics","text":"List heuristics from all backends.","title":"list_heuristics()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.list_inferencers","text":"List inferencers from all backends.","title":"list_inferencers()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.query","text":"Queries the backends for a response to the user's input. :param user_query: The user's input. :return: The backend's response.","title":"query()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_facts","text":"Select facts from all backends.","title":"select_facts()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_heuristics","text":"Select heuristics from all backends.","title":"select_heuristics()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.select_inferencers","text":"Select inferencers from all backends.","title":"select_inferencers()"},{"location":"openssm/core/adapter/base_adapter/#openssm.core.adapter.base_adapter.BaseAdapter.set_backends","text":"Set the list of backends.","title":"set_backends()"},{"location":"openssm/core/backend/abstract_backend/","text":"AbstractBackend Bases: ABC load_all () abstractmethod Loads all facts, inferencers, and heuristics, if appropriate. Some backends may not need to, and only load on demand (e.g., a database backend). query ( conversation_id , user_input ) abstractmethod Queries the backend with the user input.","title":"abstract_backend"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend","text":"Bases: ABC","title":"AbstractBackend"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.load_all","text":"Loads all facts, inferencers, and heuristics, if appropriate. Some backends may not need to, and only load on demand (e.g., a database backend).","title":"load_all()"},{"location":"openssm/core/backend/abstract_backend/#openssm.core.backend.abstract_backend.AbstractBackend.query","text":"Queries the backend with the user input.","title":"query()"},{"location":"openssm/core/backend/base_backend/","text":"BaseBackend Bases: AbstractBackend load_all () The base backend does not load anything. It gets all its facts, inferencers, and heuristics through the add_* methods. query ( conversation_id , user_input ) The base backend does not query anything. Subclasses should query the backend with the user input, and return a tuple of (a) the response string, and (b) the more informative response object, if any. select_facts ( criteria ) The base backend simply returns all facts. select_heuristics ( criteria ) The base backend simply returns all heuristics. select_inferencers ( criteria ) The base backend simply returns all inferencers.","title":"base_backend"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend","text":"Bases: AbstractBackend","title":"BaseBackend"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.load_all","text":"The base backend does not load anything. It gets all its facts, inferencers, and heuristics through the add_* methods.","title":"load_all()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.query","text":"The base backend does not query anything. Subclasses should query the backend with the user input, and return a tuple of (a) the response string, and (b) the more informative response object, if any.","title":"query()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_facts","text":"The base backend simply returns all facts.","title":"select_facts()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_heuristics","text":"The base backend simply returns all heuristics.","title":"select_heuristics()"},{"location":"openssm/core/backend/base_backend/#openssm.core.backend.base_backend.BaseBackend.select_inferencers","text":"The base backend simply returns all inferencers.","title":"select_inferencers()"},{"location":"openssm/core/backend/llama_index_backend/","text":"LlamaIndexBackend Bases: BaseBackend query ( conversation_id , user_input ) Query the index with the user input. Returns the response dicts. query2 ( conversation_id , user_input ) Query the index with the user input. Returns a tuple comprising (a) the response dicts and (b) the response object, if any.","title":"llama_index_backend"},{"location":"openssm/core/backend/llama_index_backend/#openssm.core.backend.llama_index_backend.LlamaIndexBackend","text":"Bases: BaseBackend","title":"LlamaIndexBackend"},{"location":"openssm/core/backend/llama_index_backend/#openssm.core.backend.llama_index_backend.LlamaIndexBackend.query","text":"Query the index with the user input. Returns the response dicts.","title":"query()"},{"location":"openssm/core/backend/llama_index_backend/#openssm.core.backend.llama_index_backend.LlamaIndexBackend.query2","text":"Query the index with the user input. Returns a tuple comprising (a) the response dicts and (b) the response object, if any.","title":"query2()"},{"location":"openssm/core/backend/text_backend/","text":"","title":"text_backend"},{"location":"openssm/core/inferencer/abstract_inferencer/","text":"AbstractInferencer is the base class for all inferencers. AbstractInferencer Bases: ABC The AbstractInferencer serves as the base for all concrete Inferencer classes. The most common inferencer is simply an ML model, but it could also be a rule-based system, a fuzzy logic system, or any other system that can infer a response from a given input. load ( path ) abstractmethod Loads the inferencer or its parameters from the given path. predict ( input_data ) abstractmethod Returns a prediction based on the given input.","title":"abstract_inferencer"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer","text":"Bases: ABC The AbstractInferencer serves as the base for all concrete Inferencer classes. The most common inferencer is simply an ML model, but it could also be a rule-based system, a fuzzy logic system, or any other system that can infer a response from a given input.","title":"AbstractInferencer"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer.load","text":"Loads the inferencer or its parameters from the given path.","title":"load()"},{"location":"openssm/core/inferencer/abstract_inferencer/#openssm.core.inferencer.abstract_inferencer.AbstractInferencer.predict","text":"Returns a prediction based on the given input.","title":"predict()"},{"location":"openssm/core/inferencer/base_inferencer/","text":"BaseInferencer Bases: AbstractInferencer load ( path ) The BaseInferencer does not need to load anything. predict ( input_data ) The BaseInferencer always returns a prediction of True.","title":"base_inferencer"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer","text":"Bases: AbstractInferencer","title":"BaseInferencer"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer.load","text":"The BaseInferencer does not need to load anything.","title":"load()"},{"location":"openssm/core/inferencer/base_inferencer/#openssm.core.inferencer.base_inferencer.BaseInferencer.predict","text":"The BaseInferencer always returns a prediction of True.","title":"predict()"},{"location":"openssm/core/slm/abstract_slm/","text":"AbstractSLM Bases: ABC The AbstractSLM serves as the base for all concrete Small Language Models (SLMs). It provides an interface for natural language communication and structured API interactions. discuss ( conversation_id , user_input ) abstractmethod Processes a natural language conversation input and returns a list of replies get_adapter () abstractmethod Returns our adapter reset_memory () abstractmethod Resets our conversation memory set_adapter ( adapter ) abstractmethod Sets our adapter","title":"abstract_slm"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM","text":"Bases: ABC The AbstractSLM serves as the base for all concrete Small Language Models (SLMs). It provides an interface for natural language communication and structured API interactions.","title":"AbstractSLM"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.discuss","text":"Processes a natural language conversation input and returns a list of replies","title":"discuss()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.get_adapter","text":"Returns our adapter","title":"get_adapter()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.reset_memory","text":"Resets our conversation memory","title":"reset_memory()"},{"location":"openssm/core/slm/abstract_slm/#openssm.core.slm.abstract_slm.AbstractSLM.set_adapter","text":"Sets our adapter","title":"set_adapter()"},{"location":"openssm/core/slm/base_slm/","text":"BaseSLM Bases: AbstractSLM __init__ ( adapter = None ) self.conversations is initialized as a dictionary of conversations, where each conversation is a list of user inputs and model replies. discuss ( conversation_id , user_input ) Send user input to our language model and return the replies PassthroughSLM Bases: BaseSLM The PassthroughSLM is a barebones SLM that simply passes all queries to the adapter. discuss ( conversation_id , user_input ) Pass through user input to the adapter and return the replies","title":"base_slm"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM","text":"Bases: AbstractSLM","title":"BaseSLM"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.__init__","text":"self.conversations is initialized as a dictionary of conversations, where each conversation is a list of user inputs and model replies.","title":"__init__()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.BaseSLM.discuss","text":"Send user input to our language model and return the replies","title":"discuss()"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.PassthroughSLM","text":"Bases: BaseSLM The PassthroughSLM is a barebones SLM that simply passes all queries to the adapter.","title":"PassthroughSLM"},{"location":"openssm/core/slm/base_slm/#openssm.core.slm.base_slm.PassthroughSLM.discuss","text":"Pass through user input to the adapter and return the replies","title":"discuss()"},{"location":"openssm/core/slm/huggingface_slm/","text":"This module contains the HuggingFaceBaseSLM class, and its subclasses, which are SLMs based on models from HugoingFace. The models may be served from HuggingFace's model hub, or a private internal server, or they may be served locally. Falcon7bSLM Bases: HuggingFaceBaseSLM Falcon7bSLM is a wrapper for the Falcon7b model, which may be hosted locally or remotely. If hosted remotely, the model_url and model_server_token must be provided through the Config class. FALCON7B_MODEL_URL should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If hosted locally, set to \"LOCAL\" - If not supported, set to \"NONE\" (or not set at all) Falcon7bSLMLocal Bases: Falcon7bSLM Provided for convenience, this class is a wrapper for the Falcon7b model hosted locally. HuggingFaceBaseSLM Bases: BaseSLM This class is the base class for all SLMs based on models from HuggingFace. The models may be served from HuggingFace's model hub, or a private internal server, or they may be served locally. If local, model_url must be explicitly set to \"LOCAL\". This is to avoid accidentally using a local model, which consumes a lot of resources, when the user intended to use a model from a hosted server. model_url should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If hosted locally, set to \"LOCAL\" - If not supported, set to \"NONE\" (or not set at all)","title":"huggingface_slm"},{"location":"openssm/core/slm/huggingface_slm/#openssm.core.slm.huggingface_slm.Falcon7bSLM","text":"Bases: HuggingFaceBaseSLM Falcon7bSLM is a wrapper for the Falcon7b model, which may be hosted locally or remotely. If hosted remotely, the model_url and model_server_token must be provided through the Config class. FALCON7B_MODEL_URL should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If hosted locally, set to \"LOCAL\" - If not supported, set to \"NONE\" (or not set at all)","title":"Falcon7bSLM"},{"location":"openssm/core/slm/huggingface_slm/#openssm.core.slm.huggingface_slm.Falcon7bSLMLocal","text":"Bases: Falcon7bSLM Provided for convenience, this class is a wrapper for the Falcon7b model hosted locally.","title":"Falcon7bSLMLocal"},{"location":"openssm/core/slm/huggingface_slm/#openssm.core.slm.huggingface_slm.HuggingFaceBaseSLM","text":"Bases: BaseSLM This class is the base class for all SLMs based on models from HuggingFace. The models may be served from HuggingFace's model hub, or a private internal server, or they may be served locally. If local, model_url must be explicitly set to \"LOCAL\". This is to avoid accidentally using a local model, which consumes a lot of resources, when the user intended to use a model from a hosted server. model_url should be set appropriately: - If hosted on HuggingFace, set to the model's URL on HuggingFace. - If hosted on AWS/GCP, set to the model's URL on there - If hosted locally, set to \"LOCAL\" - If not supported, set to \"NONE\" (or not set at all)","title":"HuggingFaceBaseSLM"},{"location":"openssm/core/slm/openai_slm/","text":"","title":"openai_slm"},{"location":"openssm/core/slm/memory/conversation_db/","text":"","title":"conversation_db"},{"location":"openssm/core/slm/memory/sqlite_conversation_db/","text":"","title":"sqlite_conversation_db"},{"location":"openssm/core/ssm/abstract_ssm/","text":"AbstractSSM Bases: ABC The AbstractSSM serves as the base for all concrete Small Specialist Models (SSMs). add_knowledge ( knowledge_source_uri , knowledge_type = None ) abstractmethod Uploads a knowledge source (documents, text, files, etc.) api_call ( function_name , * args , ** kwargs ) abstractmethod Processes a structured API call. discuss ( conversation_id , user_input ) abstractmethod Processes a natural language conversation input. get_adapter () abstractmethod Returns our adapter get_backends () abstractmethod Returns our backends get_slm () abstractmethod Returns our small language model (SLM) infer ( input_facts ) abstractmethod Makes inferences based on the provided input facts. list_facts () abstractmethod Lists all known facts. list_heuristics () abstractmethod Lists all known heuristics. list_inferencers () abstractmethod Lists all known inferencers. reset_memory () abstractmethod Resets the conversation memory of the SSM. select_facts ( criteria ) abstractmethod Selects or searches for facts based on provided criteria. select_heuristics ( criteria ) abstractmethod Selects or searches for heuristics based on provided criteria. select_inferencers ( criteria ) abstractmethod Selects or searches for inferencers based on provided criteria. solve_problem ( problem_description ) abstractmethod Solves a problem based on the provided description.","title":"abstract_ssm"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM","text":"Bases: ABC The AbstractSSM serves as the base for all concrete Small Specialist Models (SSMs).","title":"AbstractSSM"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.api_call","text":"Processes a structured API call.","title":"api_call()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.discuss","text":"Processes a natural language conversation input.","title":"discuss()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.get_adapter","text":"Returns our adapter","title":"get_adapter()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.get_backends","text":"Returns our backends","title":"get_backends()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.get_slm","text":"Returns our small language model (SLM)","title":"get_slm()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.infer","text":"Makes inferences based on the provided input facts.","title":"infer()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.list_facts","text":"Lists all known facts.","title":"list_facts()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.list_heuristics","text":"Lists all known heuristics.","title":"list_heuristics()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.list_inferencers","text":"Lists all known inferencers.","title":"list_inferencers()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.reset_memory","text":"Resets the conversation memory of the SSM.","title":"reset_memory()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_facts","text":"Selects or searches for facts based on provided criteria.","title":"select_facts()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_heuristics","text":"Selects or searches for heuristics based on provided criteria.","title":"select_heuristics()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.select_inferencers","text":"Selects or searches for inferencers based on provided criteria.","title":"select_inferencers()"},{"location":"openssm/core/ssm/abstract_ssm/#openssm.core.ssm.abstract_ssm.AbstractSSM.solve_problem","text":"Solves a problem based on the provided description.","title":"solve_problem()"},{"location":"openssm/core/ssm/abstract_ssm_builder/","text":"AbstractSSMBuilder Bases: ABC add_inferencer ( inferencer , knowledge_id ) abstractmethod Adds or creates an inferencer (e.g., ML models) to a specific knowledge source add_knowledge ( knowledge_source_uri , source_type = None ) abstractmethod Uploads a knowledge source (documents, text, files, etc.), returning knowledge_id create_ssm ( knowledge_ids , model_parameters = None ) abstractmethod Creates an SSM based on the provided knowledge sources and model parameters extract_structured_information ( knowledge_id ) abstractmethod Extracts structured information (facts, heuristics) from a specific knowledge_id generate_training_data ( knowledge_id , prompt_parameters = None ) abstractmethod Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model train_slm ( model , training_data , fine_tuning_parameters = None ) abstractmethod Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"abstract_ssm_builder"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder","text":"Bases: ABC","title":"AbstractSSMBuilder"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.add_inferencer","text":"Adds or creates an inferencer (e.g., ML models) to a specific knowledge source","title":"add_inferencer()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.), returning knowledge_id","title":"add_knowledge()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.create_ssm","text":"Creates an SSM based on the provided knowledge sources and model parameters","title":"create_ssm()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.extract_structured_information","text":"Extracts structured information (facts, heuristics) from a specific knowledge_id","title":"extract_structured_information()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.generate_training_data","text":"Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model","title":"generate_training_data()"},{"location":"openssm/core/ssm/abstract_ssm_builder/#openssm.core.ssm.abstract_ssm_builder.AbstractSSMBuilder.train_slm","text":"Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"train_slm()"},{"location":"openssm/core/ssm/base_ssm/","text":"BaseSSM Bases: AbstractSSM add_knowledge ( knowledge_source_uri , knowledge_type = None ) Uploads a knowledge source (documents, text, files, etc.) get_adapter () Return the previous assigned Adapter, or a default Adapter if none was assigned. get_backends () Return the previous assigned backends, or a default backend if none was assigned. get_slm () Return the previous assigned SLM, or a default SLM if none was assigned.","title":"base_ssm"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM","text":"Bases: AbstractSSM","title":"BaseSSM"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.get_adapter","text":"Return the previous assigned Adapter, or a default Adapter if none was assigned.","title":"get_adapter()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.get_backends","text":"Return the previous assigned backends, or a default backend if none was assigned.","title":"get_backends()"},{"location":"openssm/core/ssm/base_ssm/#openssm.core.ssm.base_ssm.BaseSSM.get_slm","text":"Return the previous assigned SLM, or a default SLM if none was assigned.","title":"get_slm()"},{"location":"openssm/core/ssm/base_ssm_builder/","text":"BaseSSMBuilder Bases: AbstractSSMBuilder add_inferencer ( inferencer , knowledge_id ) Adds or creates an inferencer (e.g., ML models) to a specific knowledge source add_knowledge ( knowledge_source_uri , source_type = None ) Uploads a knowledge source (documents, text, files, etc.) create_ssm ( knowledge_ids , model_parameters = None ) Creates an SSM based on the provided knowledge sources and model parameters extract_structured_information ( knowledge_id ) Extracts structured information (facts, heuristics) from a specific knowledge source generate_training_data ( knowledge_id , prompt_parameters = None ) Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model train_slm ( model , training_data , fine_tuning_parameters = None ) Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"base_ssm_builder"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder","text":"Bases: AbstractSSMBuilder","title":"BaseSSMBuilder"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.add_inferencer","text":"Adds or creates an inferencer (e.g., ML models) to a specific knowledge source","title":"add_inferencer()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.add_knowledge","text":"Uploads a knowledge source (documents, text, files, etc.)","title":"add_knowledge()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.create_ssm","text":"Creates an SSM based on the provided knowledge sources and model parameters","title":"create_ssm()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.extract_structured_information","text":"Extracts structured information (facts, heuristics) from a specific knowledge source","title":"extract_structured_information()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.generate_training_data","text":"Generates instruction-following prompts from a specific knowledge source for fine-tuning a generic large model","title":"generate_training_data()"},{"location":"openssm/core/ssm/base_ssm_builder/#openssm.core.ssm.base_ssm_builder.BaseSSMBuilder.train_slm","text":"Fine-tunes a model based on the provided training data and fine-tuning parameters. Distills a large model into a smaller model based on the provided distillation parameters.","title":"train_slm()"},{"location":"openssm/core/ssm/huggingface_ssm/","text":"","title":"huggingface_ssm"},{"location":"openssm/core/ssm/llama_index_ssm/","text":"","title":"llama_index_ssm"},{"location":"openssm/core/ssm/openai_ssm/","text":"","title":"openai_ssm"},{"location":"openssm/deprecated/chatssm-vinh/ssms/semiconductor_ssm/EMPTY/","text":"This directory is (still) empty.","title":"semiconductor_ssm"},{"location":"openssm/deprecated/chatssm-vinh/static/images/EMPTY/","text":"This directory is (still) empty.","title":"images"},{"location":"openssm/industrial/interpretability/EMPTY/","text":"This directory is (still) empty.","title":"interpretability"},{"location":"openssm/industrial/monitoring/EMPTY/","text":"This directory is (still) empty.","title":"monitoring"},{"location":"openssm/industrial/security/audit/EMPTY/","text":"This directory is (still) empty.","title":"audit"},{"location":"openssm/industrial/security/best_practices/EMPTY/","text":"This directory is (still) empty.","title":"best_practices"},{"location":"openssm/integration/llamaindex/EMPTY/","text":"This directory is (still) empty.","title":"llamaindex"},{"location":"openssm/integration/testing_tools/EMPTY/","text":"This directory is (still) empty.","title":"testing_tools"},{"location":"openssm/utils/config/","text":"Config This class is used to store config setings, as well as secrets, such as API keys, tokens, etc. By default, they come from documented environment variables. But the user can override them by setting them directly in the Config object. setenv ( var_name ) staticmethod Copy the value of a config variable to an environment variable. If the variable is not set, nothing is changed.","title":"config"},{"location":"openssm/utils/config/#openssm.utils.config.Config","text":"This class is used to store config setings, as well as secrets, such as API keys, tokens, etc. By default, they come from documented environment variables. But the user can override them by setting them directly in the Config object.","title":"Config"},{"location":"openssm/utils/config/#openssm.utils.config.Config.setenv","text":"Copy the value of a config variable to an environment variable. If the variable is not set, nothing is changed.","title":"setenv()"},{"location":"openssm/utils/logging/","text":"Logging do_log_entry ( * extra_args , log_level = logging . DEBUG ) staticmethod Decorator to log function entry. do_log_entry_and_exit ( * extra_args , logger = None , log_level = logging . DEBUG , log_entry = True , log_exit = True ) staticmethod Decorator to log function entry and exit. do_log_exit ( * extra_args , log_level = logging . DEBUG ) staticmethod Decorator to log function exit.","title":"logging"},{"location":"openssm/utils/logging/#openssm.utils.logging.Logging","text":"","title":"Logging"},{"location":"openssm/utils/logging/#openssm.utils.logging.Logging.do_log_entry","text":"Decorator to log function entry.","title":"do_log_entry()"},{"location":"openssm/utils/logging/#openssm.utils.logging.Logging.do_log_entry_and_exit","text":"Decorator to log function entry and exit.","title":"do_log_entry_and_exit()"},{"location":"openssm/utils/logging/#openssm.utils.logging.Logging.do_log_exit","text":"Decorator to log function exit.","title":"do_log_exit()"},{"location":"openssm/utils/utils/","text":"Utils canonicalize_query_response ( response ) staticmethod Make sure response is in the form of a list of dicts, e.g., [{\"role\": \"assistant\", \"content\": \"hello\"}]. canonicalize_user_input ( user_input ) staticmethod Make sure user_input is in the form of a list of dicts, e.g., [{\"role\": \"user\", \"content\": \"hello\"}]. do_canonicalize_query_response ( func ) staticmethod Decorator to canonicalize SSM query response. do_canonicalize_user_input ( param_name ) staticmethod Decorator to canonicalize SSM user input.","title":"utils"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils","text":"","title":"Utils"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.canonicalize_query_response","text":"Make sure response is in the form of a list of dicts, e.g., [{\"role\": \"assistant\", \"content\": \"hello\"}].","title":"canonicalize_query_response()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.canonicalize_user_input","text":"Make sure user_input is in the form of a list of dicts, e.g., [{\"role\": \"user\", \"content\": \"hello\"}].","title":"canonicalize_user_input()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.do_canonicalize_query_response","text":"Decorator to canonicalize SSM query response.","title":"do_canonicalize_query_response()"},{"location":"openssm/utils/utils/#openssm.utils.utils.Utils.do_canonicalize_user_input","text":"Decorator to canonicalize SSM user input.","title":"do_canonicalize_user_input()"},{"location":"support/","text":"Resources for model support and maintenance","title":"Support"},{"location":"support/#resources-for-model-support-and-maintenance","text":"","title":"Resources for model support and maintenance"},{"location":"support/FAQ/","text":"","title":"FAQ"},{"location":"support/troubleshooting_guides/","text":"","title":"Troubleshoot"}]}